<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="X-UA-COMPATIBLE" content="IE=edge,chrome=1"><meta name="renderer" content="webkit"><link rel="icon" type="image/ico" sizes="32x32" href="/assets/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png"><link rel="alternate" href="/rss.xml" title="穷拾の小屋" type="application/rss+xml"><link rel="alternate" href="/atom.xml" title="穷拾の小屋" type="application/atom+xml"><link rel="alternate" type="application/json" title="穷拾の小屋" href="https://blog.jongsh.top/feed.json"><link rel="preconnect" href="https://s4.zstatic.net"><link rel="preconnect" href="https://at.alicdn.com"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7CFredericka%20the%20Great:400,400italic,700,700italic%7CNoto%20Serif%20JP:400,400italic,700,700italic%7CNoto%20Serif%20SC:400,400italic,700,700italic%7CInconsolata:400,400italic,700,700italic&amp;display=swap&amp;subset=latin,latin-ext" media="none" onload="this.media='all'"><link rel="stylesheet" href="/css/app.css?v=0.4.25"><link rel="modulepreload" href="/js/chunk-3DLRVTZI.js"><link rel="modulepreload" href="/js/chunk-G6WST2TT.js"><link rel="modulepreload" href="/js/chunk-MEQLBSIV.js"><link rel="modulepreload" href="/js/chunk-T4ZN27XU.js"><link rel="modulepreload" href="/js/chunk-WIQECBEN.js"><link rel="modulepreload" href="/js/comments-RAU2KDSH.js"><link rel="modulepreload" href="/js/copy-tex-JHKIWPSS.js"><link rel="modulepreload" href="/js/post-DEG2LM3E.js"><link rel="modulepreload" href="/js/quicklink-VPIEOOQW.js"><link rel="modulepreload" href="/js/search-7R35WZ2Q.js"><link rel="modulepreload" href="/js/siteInit.js"><link rel="modulepreload" href="/js/waline-MSCX2FFD.js"><link rel="stylesheet" href="/css/comments-F4ZGS7LD.css" media="none" onload="this.media='all'"><link rel="stylesheet" href="/css/siteInit.css" media="none" onload="this.media='all'"><link rel="stylesheet" href="/css/waline-IDNZKML2.css" media="none" onload="this.media='all'"><link rel="preload" href="https://jongsh.oss-cn-beijing.aliyuncs.com/blog/cover/202511271917561.png?x-oss-process=image/format,webp" as="image" fetchpriority="high"><meta name="keywords" content="穷拾, jongsh, 博客, blog"><meta name="description" content="Talk is cheap, show me the code!"><link rel="canonical" href="https://blog.jongsh.top/stanford-cs336/bpe-tokenizer-algorithm/"><title>BPE 分词器算法原理与实现</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope="" itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">BPE 分词器算法原理与实现</h1><div class="meta"><span class="item" title="创建时间：2025-11-27 19:16:12"><span class="icon"><i class="ic i-calendar"></i></span><span class="text">发表于</span><time itemprop="dateCreated datePublished" datetime="2025-11-27T19:16:12+08:00">2025-11-27</time></span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i></span><span class="text">本文字数</span><span>26k</span><span class="text">字</span></span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i></span><span class="text">阅读时长</span><span>23 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span><span class="line"></span><span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">JONGSH'S BLOG</a></li></ul><ul class="right" id="rightNav"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div class="pjax" id="imgs"><img src="https://jongsh.oss-cn-beijing.aliyuncs.com/blog/cover/202511271917561.png?x-oss-process=image/format,webp" loading="eager" decoding="async" fetchpriority="high" alt="穷拾の小屋"></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"></path></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"></use><use xlink:href="#gentle-wave" x="48" y="3"></use><use xlink:href="#gentle-wave" x="48" y="5"></use><use xlink:href="#gentle-wave" x="48" y="7"></use></g></svg></div><div id="main-background" style="background-image: url(https://jongsh.oss-cn-beijing.aliyuncs.com/blog/others/202409152212632.png?x-oss-process=image/format,webp); position: relative;">   <main><div class="inner"><div class="pjax" id="main"><div class="article wrap"><div class="breadcrumb" itemlistelement="" itemscope="" itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i><span><a href="/">首页</a></span><i class="ic i-angle-right"></i><span class="current" itemprop="itemListElement" itemscope="itemscope" itemtype="https://schema.org/ListItem"><a href="/categories/stanford-cs336/" itemprop="item" rel="index" title="分类于斯坦福 CS336 课程"><span itemprop="name">斯坦福 CS336 课程<meta itemprop="position" content="0"></span></a></span></div><article class="post block" itemscope="itemscope" itemtype="http://schema.org/Article" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://blog.jongsh.top/stanford-cs336/bpe-tokenizer-algorithm/"><span hidden="hidden" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><meta itemprop="image" content="/assets/avatar.jpg"><meta itemprop="name" content="Jongsh"><meta itemprop="description" content=", Talk is cheap, show me the code!"></span><span hidden="hidden" itemprop="publisher" itemscope="itemscope" itemtype="http://schema.org/Organization"><meta itemprop="name" content="穷拾の小屋"></span><div class="body md" itemprop="articleBody"><h2 id="概述"><a class="anchor" href="#概述">#</a> 概述</h2>
<p>在自然语言处理领域，分词器（Tokenizer） 承担着至关重要的 “桥梁” 作用：它将人类的<strong>自然语言（字符串）<strong>转化为大型语言模型能够识别和处理的</strong>整数序列</strong>。该整数序列就是大模型的输入，而 token 指的就是这些整数数字。</p>
<p>在众多分词策略中，<strong>字节对编码（Byte Pair Encoding, BPE）</strong> 以其高效、可控的词汇表构建机制，成为了现代预训练模型（如 BERT，GPT，T5 等）的基石。BPE 成功解决了传统分词方法在处理海量词汇和未登录词时的痛点。</p>
<p>本篇博客基于<a target="_blank" rel="noopener" href="https://stanford-cs336.github.io/spring2025/"> 2025 斯坦福 CS336 课程</a>的第一章节作业要求，整理 BPE 分词器的原理与实现流程，并从零开始实现一个基础的 BPE 分词器。</p>
 <br> 
<h2 id="unicode-编码标准"><a class="anchor" href="#unicode-编码标准">#</a> Unicode 编码标准</h2>
<p>介绍 BPE 算法之前，需要先了解计算机如何保存各种字符串的。</p>
<p>Unicode 是一个行业标准，为世界上所有字符（英文字母、中文、日文等）提供统一的表示。截止至 2024 年公布的 Unicode 16.0 版本已经定义了 154998 个字符的表示。它为每一个字符分配了一个独一无二的编号，即码点（Code Point）。Unicode 规范了字符集，而具体的存储和传输则依赖于不同的编码方案，其中最常用的是 UTF-8。UTF-8 属于变长编码，它使用 1 到 4 个字节来表示一个 Unicode 字符。对于 ASCII 字符（如英文字母和数字），它只占用 1 个字节。</p>
<p>Python 对 Unicode 和各种编码提供了强大且直观的支持。在实现 BPE 时，主要用到以下两个核心方法：</p>
<ul>
<li>
<p>编码：字符串对象的  <code>.encode()</code>  方法将 Unicode 字符串转成对应的字节串（ <code>bytes</code>  对象）。例如要获取一个字符串的 UTF-8 字节表示，可以使用  <code>string.encode(encoding='utf-8')</code> 。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"✅"</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># b'\xe2\x9c\x85'</span></pre></td></tr></tbody></table></figure></li>
<li>
<p>解码：使用字节串对象的  <code>.decode()</code>  方法可以将字节串转换回 Unicode 字符串。例如按照 UTF-8 标准进行解码可以使用  <code>bytes_object.decode(encoding='utf-8')</code> 。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">b'\xe2\x9c\x85'</span><span class="token punctuation">.</span>decode<span class="token punctuation">(</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># '✅'</span></pre></td></tr></tbody></table></figure></li>
</ul>
<br> 
<h2 id="bpe-tokenizer-训练算法"><a class="anchor" href="#bpe-tokenizer-训练算法">#</a> BPE Tokenizer 训练算法</h2>
<p>由于所有 Unicode 字符串都可以表示为字节序列，因此一个只有 256 单元（1 字节可以表示 0-255）的词表就足以表示所有字符串。但是这个词表是非常低效。比如前面一小节中，一个  <code>✅</code>  字符就被编码成 3 个字节。即便按照平均标准，一个字符也需要约 2.5 个字节来表示，这对于现代大型语言模型的输入序列长度而言是不可承受的。Transformer 架构的核心特征使其时间复杂度与输入序列长度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span> 的平方成正比，即 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>L</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(L^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。并且，输入序列越长，训练和推理所需的计算资源（时间、GPU 显存）就越多，模型维护长期记忆和全局信息的能力越会受到限制。</p>
<p><strong>BPE 算法通过学习语料库中的高频子词模式，将多个字节合并成一个具有语义信息的子词单元，从而大幅压缩序列长度</strong>。例如，如果在词表中学习到  <code>b'\xe2\x9c\x85'</code> ，那么只用一个整数就可以表示  <code>✅</code> 。</p>
<blockquote>
<p>BPE 分词器中，每一个 token 都是一段字节序列，它可能是一个字节，一个单词，也可能是一个单词的部分字节等，因此也被称为子词（subword）。</p>
</blockquote>
<p>下面，具体讲解 BPE 分词器的训练算法流程，可以分成三个步骤：词表初始化、预分词、合并子词。</p>
<h3 id="词表初始化"><a class="anchor" href="#词表初始化">#</a> 词表初始化</h3>
<p>首先，词表必须包含 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>8</mn></msup><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">2^8=256</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">256</span></span></span></span> 个元素，与所有单字节一一对应，这 256 个字节构成了 BPE 的原子单元，保证了理论上任何 Unicode 字符串（通过 UTF-8 编码）都可以被表示和分解。</p>
<p>此外，我们还需要考虑特殊 token。所谓特殊 token 是模型可识别的，拥有特定语义功能的字符串（最后也用字节串表示），不会参与正常的 BPE 合并过程。比如  <code>&lt;cls&gt;</code> 、 <code>&lt;sep&gt;</code>  在 BERT 中就是表示分类和分隔的特殊 token。这些特殊 token 在后续的编码和训练过程中会被当作一个完整的单元进行特殊处理，因此在初始化时也需要加入到词表中。</p>
<h3 id="预分词"><a class="anchor" href="#预分词">#</a> 预分词</h3>
<p>在第三步合并子词时，我们会对语料库中频繁出现的相邻子词对进行合并。例如， <code>AI</code>  这个单词中  <code>A</code>  和  <code>I</code>  各自对应一个字节，由上述初始词表可以表示成两个 token。若  <code>AI</code>  这个单词在语料库中出现次数比较多，那么  <code>A</code>  和  <code>I</code>  在第三步就会被合并成新的 token。</p>
<p>那么预分词（Pre-tokenization）在干什么呢？我们一般期望，<strong>每个 token 都应该具有语义信息</strong>，比如上述的  <code>AI</code>  亦或者是和英语语法有关的  <code>est</code> 、 <code>er</code>  等等，而对于  <code>y n</code>  这种就不是一个好的 token 表示。但是，假设说，语料库中经常出现 "my name" 文本，那么在第三步合并子词时就有可能合并出  <code>y n</code>  这种 token。因此预分词的目的就是定义子词合并的合理边界，避免跨单词合并，从而保留语义结构。</p>
<p>CS336 第一章作业中，我们使用如下 GPT-2 的预分词正则表达式，它可以将一段话拆分成具有完整含义的子字符串序列：</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> regex</pre></td></tr><tr><td data-num="2"></td><td><pre>PAT <span class="token operator">=</span> <span class="token triple-quoted-string string">r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""</span></pre></td></tr><tr><td data-num="3"></td><td><pre>regex<span class="token punctuation">.</span>findall<span class="token punctuation">(</span>PAT<span class="token punctuation">,</span> <span class="token string">"你好世界, Hello World!"</span><span class="token punctuation">)</span>  <span class="token comment"># [' 你好世界 ', ',', ' Hello', ' World', '!']</span></pre></td></tr></tbody></table></figure><p>这里划分出来的子字符串列表就是就是下一步 BPE 合并操作的最小作用域。</p>
<p>预分词不仅可以保证合并出来的子词具有语义结构，还可以提高 BPE 训练的效率。在这一阶段，我们可以统计出每个预分词块（例如  <code>Hello</code> ）在整个语料库中出现的总次数。在后续的合并计数中，我们只需将内部子词对的频率乘以该块的出现次数，即可避免对整个语料进行重复扫描，大大加速合并计数。</p>
<h3 id="子词合并"><a class="anchor" href="#子词合并">#</a> 子词合并</h3>
<p>子词合并就是 BPE 训练算法最核心的部分。</p>
<p>对于每个预分词后的子字符串，我们可以用当前的词表将其表示为一系列子词，进而得到整个语料库的子词表示。然后统计所有相邻的子词对的出现次数。注意，我们不统计预分词子字符串之间的子词对。然后将出现次数最多，字典序最大的子词对取出进行合并，得到新的子词添加到词表中，进而用新的子词替换原先出现的子词对。</p>
<blockquote>
<p>BPE 中，子词就是 token，在词表中用字节串表示。</p>
</blockquote>
<p>例如，在下面语料中，第一次合并时选择的子词对是  <code>(b's', b't')</code> ，进而我们将  <code>b'st'</code>  添加到词表中，然后将  <code>[b'n', b'e', b'w', b'e', b's', b't']</code>  重新表示为  <code>[b'n', b'e', b'w', b'e', b'st']</code> 。</p>
<figure class="highlight txt"><figcaption data-lang="txt"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>low low low low low</pre></td></tr><tr><td data-num="2"></td><td><pre>lower lower widest widest widest</pre></td></tr><tr><td data-num="3"></td><td><pre>newest newest newest newest newest newest</pre></td></tr></tbody></table></figure><p>上述过程会不断迭代进行，直至词表达到预定义的大小。</p>
<br>
<h2 id="bpe-tokenizer-编码解码"><a class="anchor" href="#bpe-tokenizer-编码解码">#</a> BPE Tokenizer 编码解码</h2>
<p>在 BPE 算法训练结束后，我们得到了两份数据：</p>
<ol>
<li><strong>词汇表（ <code>vocab</code> ）：</strong> 包含了所有的基本字节和学习到的复合子词。</li>
<li><strong>合并规则序列（ <code>merges</code> ）：</strong> 记录了 BPE 训练过程中，所有子词对被合并的顺序。</li>
</ol>
<p>基于这两个数据，我们可以构建一个完整的  <code>BPETokenizer</code>  类，其核心功能是实现将自然语言转化为模型输入的 <strong> <code>encode()</code> </strong> 方法，以及反向的 <strong> <code>decode()</code> </strong> 方法。前者将给定的 Unicode 字符串转换为模型可识别的整数 Token ID 序列，后者则将给定的整数 Token ID 序列转换成 Unicode 字符串。</p>
<p>给定词表和待编码的 Unicode 字符串，其实有许多可行的编码方案。比如最简单的一种就是每个字节都编码成一个 token。但为了确保模型输入的一致性和效率，我们必须遵循一个最优的、确定的编码方案。按照 CS336 课程作业要求， <code>BPETokenizer</code>  的编码流程如下：</p>
<ul>
<li>首先，使用与训练时完全相同的预分词方法，将输入字符串拆分成语义子字符串块，并将其转化为 UTF-8 字节序列，并初始化为一个字节一个 Token 的序列。</li>
<li>严格按照训练时学习到的  <code>merges</code>  列表的顺序，从头到尾遍历每条合并规则。对于当前序列中的每一个预分词块，如果其中包含当前规则要合并的子词对，则进行合并，最终生成一个新的 Token ID 序列。</li>
<li>当  <code>merges</code>  列表中的所有规则都应用完毕后，最终得到的 Token 序列就是我们期望的、压缩效率最高的编码结果。</li>
</ul>
<p>上述编码流程完全仿照训练过程。这是因为 BPE 算法的本质是<strong>贪婪的</strong>，它学习到的  <code>merges</code>  列表反映了将序列压缩到词汇表大小限制内的<strong>最优、最频繁的</strong>合并路径。尽管对于单个待编码的 Unicode 字符串来说，通过这种方式编码得到的 Token 数目不一定是理论上最少的。但是，由于  <code>merges</code>  列表是基于整个大规模训练语料统计的最高频模式学习而来，因此在整个输入空间上，这种编码方式具有<strong>最优的期望压缩效率</strong>。</p>
<p>解码是一个相对直接的反向过程：遍历输入的整数序列，根据保存的  <code>vocab</code>  词汇表，查找每个 Token ID 对应的字节序列。将所有查找到的字节序列按顺序拼接成一个完整的字节串对象。最后使用字节串对象的  <code>.decode(encoding='utf-8')</code>  方法转成最终的 Unicode 字符串。</p>
<br>  
<h2 id="代码实现"><a class="anchor" href="#代码实现">#</a> 代码实现</h2>
<p>本节将按照<a target="_blank" rel="noopener" href="https://stanford-cs336.github.io/spring2025/"> 2025 斯坦福 CS336 课程</a>的作业要求，从零实现一个基础的 BPE 分词器，并补充相关的技术细节。</p>
<h3 id="训练语料"><a class="anchor" href="#训练语料">#</a> 训练语料</h3>
<p>下面是代码实现中用到的语料数据。</p>
<figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token function">mkdir</span> <span class="token parameter variable">-p</span> data</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token builtin class-name">cd</span> data</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token function">wget</span> https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token function">wget</span> https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt</pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token function">wget</span> https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_train.txt.gz</pre></td></tr><tr><td data-num="8"></td><td><pre>gunzip owt_train.txt.gz</pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token function">wget</span> https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_valid.txt.gz</pre></td></tr><tr><td data-num="10"></td><td><pre>gunzip owt_valid.txt.gz</pre></td></tr></tbody></table></figure><p>这些语料数据已经按照要求经过了预处理，可以直接用来训练 BPE Tokenizer。 <code>valid</code>  数据相比  <code>train</code>  数据少得多，用于快速验证算法的正确性。</p>
<h3 id="目录结构"><a class="anchor" href="#目录结构">#</a> 目录结构</h3>
<p>所有代码总共分成四个文件，如下所示：</p>
<figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>- pretokenize.py</pre></td></tr><tr><td data-num="2"></td><td><pre>- train.py</pre></td></tr><tr><td data-num="3"></td><td><pre>- tokenizer.py</pre></td></tr><tr><td data-num="4"></td><td><pre>- utils.py</pre></td></tr></tbody></table></figure><p><code>pretokenize.py</code>  保存与预分词相关的函数。</p>
<p><code>train.py</code>  保存训练 BPE Tokenizer 的函数。</p>
<p><code>tokenizer.py</code>  定义了 BPE Tokenizer 类，实现编码解码功能。</p>
<p><code>utils.py</code>  保存了整个 BPE 算法实现过程中用到的数据结构。</p>
<h3 id="bpe-训练函数"><a class="anchor" href="#bpe-训练函数">#</a> BPE 训练函数</h3>
<h4 id="工具模块"><a class="anchor" href="#工具模块">#</a> 工具模块</h4>
<p>首先给出  <code>utils.py</code>  的文件内容，然后详细说明各部分的作用。</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># utils.py</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> typing <span class="token keyword">import</span> NamedTuple<span class="token punctuation">,</span> List<span class="token punctuation">,</span> Tuple</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">from</span> dataclasses <span class="token keyword">import</span> dataclass<span class="token punctuation">,</span> field</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">from</span> collections <span class="token keyword">import</span> Counter<span class="token punctuation">,</span> defaultdict</pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre>BYTES<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">bytes</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">bytes</span><span class="token punctuation">(</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="7"></td><td><pre>PAT <span class="token operator">=</span> <span class="token triple-quoted-string string">r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""</span></pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token keyword">def</span> <span class="token function">split_bytes</span><span class="token punctuation">(</span>data<span class="token punctuation">:</span> <span class="token builtin">bytes</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tuple<span class="token punctuation">[</span><span class="token builtin">bytes</span><span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    <span class="token triple-quoted-string string">"""Split a bytestring into a list of single-byte bytestrings."""</span></pre></td></tr><tr><td data-num="11"></td><td><pre>    <span class="token keyword">return</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span><span class="token punctuation">[</span>BYTES<span class="token punctuation">[</span>b<span class="token punctuation">]</span> <span class="token keyword">for</span> b <span class="token keyword">in</span> data<span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p><code>BYTES</code>  保存了 256 个单字节对象，避免在分词时频繁创建新的字节对象，减少内存消耗。 <code>PAT</code>  是预分词使用的正则表达式字符串。</p>
<p><code>split_bytes</code>  函数将字节串拆分成成单字节对象组成的元组。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># utils.py</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">class</span> <span class="token class-name">myHeap</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">"max"</span><span class="token punctuation">,</span> init_list<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="4"></td><td><pre>        self<span class="token punctuation">.</span>mode <span class="token operator">=</span> mode</pre></td></tr><tr><td data-num="5"></td><td><pre>        self<span class="token punctuation">.</span>_init_data<span class="token punctuation">(</span>init_list <span class="token keyword">or</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">_init_data</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> init_list<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">list</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="8"></td><td><pre>        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>init_list<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>        self<span class="token punctuation">.</span>size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>size <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="11"></td><td><pre>            <span class="token keyword">return</span></pre></td></tr><tr><td data-num="12"></td><td><pre>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="13"></td><td><pre>            self<span class="token punctuation">.</span>_shift_down<span class="token punctuation">(</span>i<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="14"></td><td><pre></pre></td></tr><tr><td data-num="15"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">_compare</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="16"></td><td><pre>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>mode <span class="token operator">==</span> <span class="token string">"max"</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="17"></td><td><pre>            <span class="token keyword">return</span> a <span class="token operator">&gt;</span> b</pre></td></tr><tr><td data-num="18"></td><td><pre>        <span class="token keyword">else</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="19"></td><td><pre>            <span class="token keyword">return</span> a <span class="token operator">&lt;</span> b</pre></td></tr><tr><td data-num="20"></td><td><pre></pre></td></tr><tr><td data-num="21"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">_shift_down</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="22"></td><td><pre>        <span class="token triple-quoted-string string">"""Shift down the element at index to maintain heap property"""</span></pre></td></tr><tr><td data-num="23"></td><td><pre>        <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="24"></td><td><pre>            left <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> index <span class="token operator">+</span> <span class="token number">1</span></pre></td></tr><tr><td data-num="25"></td><td><pre>            right <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> index <span class="token operator">+</span> <span class="token number">2</span></pre></td></tr><tr><td data-num="26"></td><td><pre>            maxIndex <span class="token operator">=</span> index</pre></td></tr><tr><td data-num="27"></td><td><pre>            <span class="token keyword">if</span> left <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>size <span class="token keyword">and</span> self<span class="token punctuation">.</span>_compare<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>left<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>maxIndex<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="28"></td><td><pre>                maxIndex <span class="token operator">=</span> left</pre></td></tr><tr><td data-num="29"></td><td><pre>            <span class="token keyword">if</span> right <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>size <span class="token keyword">and</span> self<span class="token punctuation">.</span>_compare<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>right<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>maxIndex<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="30"></td><td><pre>                maxIndex <span class="token operator">=</span> right</pre></td></tr><tr><td data-num="31"></td><td><pre>            <span class="token keyword">if</span> maxIndex <span class="token operator">!=</span> index<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="32"></td><td><pre>                self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>maxIndex<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>maxIndex<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>index<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="33"></td><td><pre>                index <span class="token operator">=</span> maxIndex</pre></td></tr><tr><td data-num="34"></td><td><pre>            <span class="token keyword">else</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="35"></td><td><pre>                <span class="token keyword">break</span></pre></td></tr><tr><td data-num="36"></td><td><pre></pre></td></tr><tr><td data-num="37"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">push</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="38"></td><td><pre>        <span class="token triple-quoted-string string">"""Push an item onto the heap"""</span></pre></td></tr><tr><td data-num="39"></td><td><pre>        self<span class="token punctuation">.</span>data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>item<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="40"></td><td><pre>        self<span class="token punctuation">.</span>size <span class="token operator">+=</span> <span class="token number">1</span></pre></td></tr><tr><td data-num="41"></td><td><pre>        index <span class="token operator">=</span> self<span class="token punctuation">.</span>size <span class="token operator">-</span> <span class="token number">1</span></pre></td></tr><tr><td data-num="42"></td><td><pre>        <span class="token keyword">while</span> index <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="43"></td><td><pre>            parent <span class="token operator">=</span> <span class="token punctuation">(</span>index <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span></pre></td></tr><tr><td data-num="44"></td><td><pre>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>_compare<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>parent<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="45"></td><td><pre>                self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>parent<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>parent<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>index<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="46"></td><td><pre>                index <span class="token operator">=</span> parent</pre></td></tr><tr><td data-num="47"></td><td><pre>            <span class="token keyword">else</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="48"></td><td><pre>                <span class="token keyword">break</span></pre></td></tr><tr><td data-num="49"></td><td><pre></pre></td></tr><tr><td data-num="50"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">pop</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">any</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="51"></td><td><pre>        <span class="token triple-quoted-string string">"""Pop the top item off the heap and return it"""</span></pre></td></tr><tr><td data-num="52"></td><td><pre>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>size <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="53"></td><td><pre>            <span class="token keyword">return</span> <span class="token boolean">None</span></pre></td></tr><tr><td data-num="54"></td><td><pre>        top_item <span class="token operator">=</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="55"></td><td><pre>        self<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="56"></td><td><pre>        self<span class="token punctuation">.</span>data<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="57"></td><td><pre>        self<span class="token punctuation">.</span>size <span class="token operator">-=</span> <span class="token number">1</span></pre></td></tr><tr><td data-num="58"></td><td><pre>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>size <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="59"></td><td><pre>            self<span class="token punctuation">.</span>_shift_down<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="60"></td><td><pre>        <span class="token keyword">return</span> top_item</pre></td></tr><tr><td data-num="61"></td><td><pre></pre></td></tr><tr><td data-num="62"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">top</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">any</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="63"></td><td><pre>        <span class="token triple-quoted-string string">"""Return the top item of the heap without removing it"""</span></pre></td></tr><tr><td data-num="64"></td><td><pre>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>size <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">else</span> <span class="token boolean">None</span></pre></td></tr></tbody></table></figure><p><code>myHeap</code>  类是手动实现的堆数据结构，支持大顶堆和小顶堆，用于高效获取频率最高，字典序最大的分词对。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># utils.py</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">class</span> <span class="token class-name">BytePair</span><span class="token punctuation">(</span>NamedTuple<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    left<span class="token punctuation">:</span> <span class="token builtin">bytes</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    right<span class="token punctuation">:</span> <span class="token builtin">bytes</span></pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre>    <span class="token decorator annotation punctuation">@property</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">merged_bytes</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="8"></td><td><pre>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>left <span class="token operator">+</span> self<span class="token punctuation">.</span>right</pre></td></tr><tr><td data-num="9"></td><td><pre></pre></td></tr><tr><td data-num="10"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">__str__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">str</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="11"></td><td><pre>        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"(</span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>left<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">,</span> errors<span class="token operator">=</span><span class="token string">'ignore'</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>right<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">,</span> errors<span class="token operator">=</span><span class="token string">'ignore'</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">)"</span></span></pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">__repr__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">str</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="14"></td><td><pre>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>__str__<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p>BPE Tokenizer 中每个 token 都是一个字节串，每次合并时需要选择一个 token 对。为了方便管理，这里定义  <code>BytePair</code>  类，继承元组类型，表示 token 对，其中  <code>merged_bytes</code>  方法通过修饰器转换成属性，可以返回该 token 对合并后的字节串。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># utils.py</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token decorator annotation punctuation">@dataclass</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">class</span> <span class="token class-name">WordRef</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    tokens<span class="token punctuation">:</span> <span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">bytes</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    count<span class="token punctuation">:</span> <span class="token builtin">int</span></pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre>    <span class="token decorator annotation punctuation">@property</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">byte_pairs_dict</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> defaultdict<span class="token punctuation">[</span>BytePair<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="9"></td><td><pre>        bp_to_count <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>tokens<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="11"></td><td><pre>            bp <span class="token operator">=</span> BytePair<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tokens<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>tokens<span class="token punctuation">[</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre>            bp_to_count<span class="token punctuation">[</span>bp<span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>count</pre></td></tr><tr><td data-num="13"></td><td><pre></pre></td></tr><tr><td data-num="14"></td><td><pre>        <span class="token keyword">return</span> bp_to_count</pre></td></tr><tr><td data-num="15"></td><td><pre></pre></td></tr><tr><td data-num="16"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">merge</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> bp_merged<span class="token punctuation">:</span> BytePair<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="17"></td><td><pre>        <span class="token triple-quoted-string string">"""merge the given byte pair in this word"""</span></pre></td></tr><tr><td data-num="18"></td><td><pre>        new_tokens<span class="token punctuation">:</span> <span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">bytes</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="19"></td><td><pre>        i <span class="token operator">=</span> <span class="token number">0</span></pre></td></tr><tr><td data-num="20"></td><td><pre>        <span class="token keyword">while</span> i <span class="token operator">&lt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>tokens<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="21"></td><td><pre>            <span class="token keyword">if</span> i <span class="token operator">&lt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>tokens<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span> <span class="token keyword">and</span> self<span class="token punctuation">.</span>tokens<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> bp_merged<span class="token punctuation">.</span>left <span class="token keyword">and</span> self<span class="token punctuation">.</span>tokens<span class="token punctuation">[</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> bp_merged<span class="token punctuation">.</span>right<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="22"></td><td><pre>                new_tokens<span class="token punctuation">.</span>append<span class="token punctuation">(</span>bp_merged<span class="token punctuation">.</span>merged_bytes<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="23"></td><td><pre>                i <span class="token operator">+=</span> <span class="token number">2</span></pre></td></tr><tr><td data-num="24"></td><td><pre>            <span class="token keyword">else</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="25"></td><td><pre>                new_tokens<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tokens<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="26"></td><td><pre>                i <span class="token operator">+=</span> <span class="token number">1</span></pre></td></tr><tr><td data-num="27"></td><td><pre>        self<span class="token punctuation">.</span>tokens <span class="token operator">=</span> new_tokens</pre></td></tr><tr><td data-num="28"></td><td><pre></pre></td></tr><tr><td data-num="29"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">__str__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">str</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="30"></td><td><pre>        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>tokens<span class="token punctuation">}</span></span><span class="token string"> : </span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>count<span class="token punctuation">}</span></span><span class="token string">"</span></span></pre></td></tr><tr><td data-num="31"></td><td><pre></pre></td></tr><tr><td data-num="32"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">__repr__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">str</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="33"></td><td><pre>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>__str__<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p><code>WordRef</code>  类表示预分词后，每个有效的子字节串。属性  <code>tokens</code>  表示当前该字节串被表示成的 token 序列，属性  <code>cnt</code>  是预分词时统计该子字节串出现的次数。</p>
<p>由于 BPE 算法合并过程时需要统计 token 对数量，因此这里定义  <code>byte_pairs_dict</code>  返回该字节串的对应的 token 对统计结果。 <code>merge</code>  函数接受需要合并的 token 对，类型为  <code>BytePair</code>  对象，按要求合并，得到新的 token 序列。</p>
<h4 id="预分词模块"><a class="anchor" href="#预分词模块">#</a> 预分词模块</h4>
<p>介绍完  <code>utils.py</code>  的内容，再来讲讲预分词的代码如何实现。</p>
<p>预分词的功能说起来简单，只需要使用前文提到的正则表达式进行匹配得到结果即可。但难点在于如何兼备内存效率和计算效率。一般来说，BPE Tokenizer 训练的语料非常庞大， <code>owt_train.txt</code>  都有 11G，更别提现代大模型所需要的训练数据量。一次性读取所有文件内容进内存显然不合理，因此我们需要给语料数据进行分块，逐块进行预分词。</p>
<p>另外，正则表达式匹配是一个计算复杂度非常高的过程，直接对一个长文本进行预分词需要耗费非常多的时间。分块处理的另一个好处是可以使用多进程编程提高计算效率。</p>
<p>CS336 课程提供了文件分块处理的函数，如下所示：</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># pretokenize.py</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> os</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">import</span> re</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">import</span> regex</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token keyword">import</span> multiprocessing</pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token keyword">import</span> io</pre></td></tr><tr><td data-num="7"></td><td><pre></pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token keyword">from</span> typing <span class="token keyword">import</span> BinaryIO<span class="token punctuation">,</span> Dict<span class="token punctuation">,</span> Tuple<span class="token punctuation">,</span> List<span class="token punctuation">,</span> Iterable</pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token keyword">from</span> collections <span class="token keyword">import</span> Counter</pre></td></tr><tr><td data-num="10"></td><td><pre></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token keyword">from</span> cs336_basics<span class="token punctuation">.</span>bpe<span class="token punctuation">.</span>utils <span class="token keyword">import</span> split_bytes<span class="token punctuation">,</span> PAT</pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre><span class="token keyword">def</span> <span class="token function">find_chunk_boundaries</span><span class="token punctuation">(</span></pre></td></tr><tr><td data-num="14"></td><td><pre>    <span class="token builtin">file</span><span class="token punctuation">:</span> BinaryIO<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="15"></td><td><pre>    desired_num_chunks<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="16"></td><td><pre>    split_special_token<span class="token punctuation">:</span> <span class="token builtin">bytes</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="17"></td><td><pre><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="18"></td><td><pre>    <span class="token triple-quoted-string string">"""</span></pre></td></tr><tr><td data-num="19"></td><td><pre>    Chunk the file into parts that can be counted independently.</pre></td></tr><tr><td data-num="20"></td><td><pre>    May return fewer chunks if the boundaries end up overlapping.</pre></td></tr><tr><td data-num="21"></td><td><pre>    """</pre></td></tr><tr><td data-num="22"></td><td><pre>    <span class="token keyword">assert</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>split_special_token<span class="token punctuation">,</span> <span class="token builtin">bytes</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"Must represent special token as a bytestring"</span></pre></td></tr><tr><td data-num="23"></td><td><pre></pre></td></tr><tr><td data-num="24"></td><td><pre>    <span class="token comment"># Get total file size in bytes</span></pre></td></tr><tr><td data-num="25"></td><td><pre>    <span class="token builtin">file</span><span class="token punctuation">.</span>seek<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>SEEK_END<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="26"></td><td><pre>    file_size <span class="token operator">=</span> <span class="token builtin">file</span><span class="token punctuation">.</span>tell<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="27"></td><td><pre>    <span class="token builtin">file</span><span class="token punctuation">.</span>seek<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="28"></td><td><pre></pre></td></tr><tr><td data-num="29"></td><td><pre>    chunk_size <span class="token operator">=</span> file_size <span class="token operator">//</span> desired_num_chunks</pre></td></tr><tr><td data-num="30"></td><td><pre></pre></td></tr><tr><td data-num="31"></td><td><pre>    <span class="token comment"># Initial guesses for chunk boundary locations, uniformly spaced</span></pre></td></tr><tr><td data-num="32"></td><td><pre>    <span class="token comment"># Chunks start on previous index, don't include last index</span></pre></td></tr><tr><td data-num="33"></td><td><pre>    chunk_boundaries <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token operator">*</span> chunk_size <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>desired_num_chunks <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="34"></td><td><pre>    chunk_boundaries<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> file_size</pre></td></tr><tr><td data-num="35"></td><td><pre></pre></td></tr><tr><td data-num="36"></td><td><pre>    mini_chunk_size <span class="token operator">=</span> <span class="token number">4096</span>  <span class="token comment"># Read ahead by 4k bytes at a time</span></pre></td></tr><tr><td data-num="37"></td><td><pre></pre></td></tr><tr><td data-num="38"></td><td><pre>    <span class="token keyword">for</span> bi <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>chunk_boundaries<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="39"></td><td><pre>        initial_position <span class="token operator">=</span> chunk_boundaries<span class="token punctuation">[</span>bi<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="40"></td><td><pre>        <span class="token builtin">file</span><span class="token punctuation">.</span>seek<span class="token punctuation">(</span>initial_position<span class="token punctuation">)</span>  <span class="token comment"># Start at boundary guess</span></pre></td></tr><tr><td data-num="41"></td><td><pre>        <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="42"></td><td><pre>            mini_chunk <span class="token operator">=</span> <span class="token builtin">file</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span>mini_chunk_size<span class="token punctuation">)</span>  <span class="token comment"># Read a mini chunk</span></pre></td></tr><tr><td data-num="43"></td><td><pre></pre></td></tr><tr><td data-num="44"></td><td><pre>            <span class="token comment"># If EOF, this boundary should be at the end of the file</span></pre></td></tr><tr><td data-num="45"></td><td><pre>            <span class="token keyword">if</span> mini_chunk <span class="token operator">==</span> <span class="token string">b""</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="46"></td><td><pre>                chunk_boundaries<span class="token punctuation">[</span>bi<span class="token punctuation">]</span> <span class="token operator">=</span> file_size</pre></td></tr><tr><td data-num="47"></td><td><pre>                <span class="token keyword">break</span></pre></td></tr><tr><td data-num="48"></td><td><pre></pre></td></tr><tr><td data-num="49"></td><td><pre>            <span class="token comment"># Find the special token in the mini chunk</span></pre></td></tr><tr><td data-num="50"></td><td><pre>            found_at <span class="token operator">=</span> mini_chunk<span class="token punctuation">.</span>find<span class="token punctuation">(</span>split_special_token<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="51"></td><td><pre>            <span class="token keyword">if</span> found_at <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="52"></td><td><pre>                chunk_boundaries<span class="token punctuation">[</span>bi<span class="token punctuation">]</span> <span class="token operator">=</span> initial_position <span class="token operator">+</span> found_at</pre></td></tr><tr><td data-num="53"></td><td><pre>                <span class="token keyword">break</span></pre></td></tr><tr><td data-num="54"></td><td><pre>            initial_position <span class="token operator">+=</span> mini_chunk_size</pre></td></tr><tr><td data-num="55"></td><td><pre></pre></td></tr><tr><td data-num="56"></td><td><pre>    <span class="token comment"># Make sure all boundaries are unique, but might be fewer than desired_num_chunks</span></pre></td></tr><tr><td data-num="57"></td><td><pre>    <span class="token keyword">return</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">(</span>chunk_boundaries<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p>给语料数据进行分块时要注意避免一个正常的单词被切分成两块，因此需要定义一个分块的边界。这个分块的边界通常由开发人员自己定义。本作业使用到的语料数据均使用一个特殊 token  <code>&lt;|endoftext|&gt;</code>  来分割不同的文档，这也是本次作业中唯一使用到的特殊 token。</p>
<p><code>find_chunk_boundaries</code>  函数正是基于这个特殊 token 来获取每个语料块的边界。简单来说，这个函数根据文件 IO 指针计算出整个文件内容所占的字节数，然后根据预期分块数  <code>desired_num_chunks</code>  均分得到初始分块边界。针对每个边界，逐步向后读取固定字节大小的内容，检测是否出现了特殊的分割 token，更新最终的边界。由于每个初始边界都会向后调整，因此最后有效的块数可能会少于预期值，因此返回时使用  <code>sorted(set(chunk_boundaries))</code>  进行后处理。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">pretokenize_file</span><span class="token punctuation">(</span><span class="token builtin">file</span><span class="token punctuation">:</span> BinaryIO<span class="token punctuation">,</span> special_tokens<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Counter<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span><span class="token builtin">bytes</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    <span class="token triple-quoted-string string">"""</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    Pre-tokenize a file into words and count their occurrences, using multiprocessing.</pre></td></tr><tr><td data-num="4"></td><td><pre>    The primary function to execute the parallel pre-tokenization process.</pre></td></tr><tr><td data-num="5"></td><td><pre>    """</pre></td></tr><tr><td data-num="6"></td><td><pre>    <span class="token comment"># 1. prepare special tokens</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    split_special_token <span class="token operator">=</span> <span class="token string">b"&lt;|endoftext|&gt;"</span></pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre>    <span class="token comment"># 2. find chunk boundaries</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    num_processes <span class="token operator">=</span> <span class="token number">16</span></pre></td></tr><tr><td data-num="11"></td><td><pre>    chunk_max_size <span class="token operator">=</span> <span class="token number">1024</span> <span class="token operator">*</span> <span class="token number">1024</span> <span class="token operator">*</span> <span class="token number">500</span>  <span class="token comment"># 500 MB</span></pre></td></tr><tr><td data-num="12"></td><td><pre>    desired_chunks <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>num_processes<span class="token punctuation">,</span> <span class="token builtin">file</span><span class="token punctuation">.</span>seek<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>SEEK_END<span class="token punctuation">)</span> <span class="token operator">//</span> chunk_max_size <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="13"></td><td><pre>    boundaries <span class="token operator">=</span> find_chunk_boundaries<span class="token punctuation">(</span><span class="token builtin">file</span><span class="token punctuation">,</span> desired_num_chunks<span class="token operator">=</span>desired_chunks<span class="token punctuation">,</span> split_special_token<span class="token operator">=</span>split_special_token<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="14"></td><td><pre></pre></td></tr><tr><td data-num="15"></td><td><pre>    <span class="token comment"># 3. pretokenize chunks in parallel</span></pre></td></tr><tr><td data-num="16"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">chunk_generator</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="17"></td><td><pre>        <span class="token keyword">for</span> start<span class="token punctuation">,</span> end <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>boundaries<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> boundaries<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="18"></td><td><pre>            <span class="token builtin">file</span><span class="token punctuation">.</span>seek<span class="token punctuation">(</span>start<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="19"></td><td><pre>            chunk_data <span class="token operator">=</span> <span class="token builtin">file</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span>end <span class="token operator">-</span> start<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="20"></td><td><pre>            <span class="token keyword">yield</span> <span class="token punctuation">(</span>chunk_data<span class="token punctuation">,</span> special_tokens<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="21"></td><td><pre></pre></td></tr><tr><td data-num="22"></td><td><pre>    final_words_counter<span class="token punctuation">:</span> Counter<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span><span class="token builtin">bytes</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> Counter<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="23"></td><td><pre>    <span class="token keyword">try</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="24"></td><td><pre>        <span class="token keyword">with</span> multiprocessing<span class="token punctuation">.</span>Pool<span class="token punctuation">(</span>processes<span class="token operator">=</span>num_processes<span class="token punctuation">)</span> <span class="token keyword">as</span> pool<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="25"></td><td><pre>            <span class="token keyword">for</span> result <span class="token keyword">in</span> pool<span class="token punctuation">.</span>imap_unordered<span class="token punctuation">(</span>pretokenize_chunk<span class="token punctuation">,</span> chunk_generator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="26"></td><td><pre>                final_words_counter<span class="token punctuation">.</span>update<span class="token punctuation">(</span>result<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="27"></td><td><pre>    <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="28"></td><td><pre>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Multiprocessing failed, falling back to single process: </span><span class="token interpolation"><span class="token punctuation">{</span>e<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="29"></td><td><pre>        <span class="token keyword">for</span> chunk_data<span class="token punctuation">,</span> special_tokens <span class="token keyword">in</span> chunk_generator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="30"></td><td><pre>            counter <span class="token operator">=</span> pretokenize_chunk<span class="token punctuation">(</span><span class="token punctuation">(</span>chunk_data<span class="token punctuation">,</span> special_tokens<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="31"></td><td><pre>            final_words_counter<span class="token punctuation">.</span>update<span class="token punctuation">(</span>counter<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="32"></td><td><pre></pre></td></tr><tr><td data-num="33"></td><td><pre>    <span class="token keyword">return</span> final_words_counter</pre></td></tr></tbody></table></figure><p><code>pretokenize_file</code>  函数为整个文件内容进行预分词，返回预分词后每个子字节串的统计结果。注意这里的子字节串被表示为单字节元组，用于后续创建  <code>WordRef</code>  对象。</p>
<p>该函数使用了多进程编程技术，定义每个分块的最大尺寸和可用进程数，调用  <code>find_chunk_boundaries</code>  获取每个分块的边界。然后通过  <code>chunk_generator</code>  迭代器动态读取文件指定分块内容进入内存。接着使用进程池和  <code>pool.imap_unordered</code>  分发预分词任务，每个子进程自行处理对应的分块语料内容。</p>
<p>子进程预分词的逻辑为  <code>pretokenize_chunk</code>  函数，具体实现如下：</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">pretokenize_chunk</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Counter<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span><span class="token builtin">bytes</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    <span class="token triple-quoted-string string">"""</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    Pre-tokenize a chunk, replace special tokens, and count word occurrences.</pre></td></tr><tr><td data-num="4"></td><td><pre>    This function runs in parallel worker processes.</pre></td></tr><tr><td data-num="5"></td><td><pre>    """</pre></td></tr><tr><td data-num="6"></td><td><pre>    chunk<span class="token punctuation">,</span> special_tokens <span class="token operator">=</span> args</pre></td></tr><tr><td data-num="7"></td><td><pre>    chunk_str <span class="token operator">=</span> chunk<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">,</span> errors<span class="token operator">=</span><span class="token string">"ignore"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    words_counter <span class="token operator">=</span> Counter<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    last_end <span class="token operator">=</span> <span class="token number">0</span></pre></td></tr><tr><td data-num="10"></td><td><pre></pre></td></tr><tr><td data-num="11"></td><td><pre>    <span class="token comment"># pretokenize between special tokens</span></pre></td></tr><tr><td data-num="12"></td><td><pre>    <span class="token keyword">if</span> special_tokens <span class="token keyword">and</span> <span class="token builtin">len</span><span class="token punctuation">(</span>special_tokens<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="13"></td><td><pre>        <span class="token keyword">for</span> <span class="token keyword">match</span> <span class="token keyword">in</span> regex<span class="token punctuation">.</span>finditer<span class="token punctuation">(</span><span class="token string">"|"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>re<span class="token punctuation">.</span>escape<span class="token punctuation">(</span>st<span class="token punctuation">)</span> <span class="token keyword">for</span> st <span class="token keyword">in</span> special_tokens<span class="token punctuation">)</span><span class="token punctuation">,</span> chunk_str<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="14"></td><td><pre>            pre_token_text <span class="token operator">=</span> chunk_str<span class="token punctuation">[</span>last_end <span class="token punctuation">:</span> <span class="token keyword">match</span><span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="15"></td><td><pre>            <span class="token keyword">for</span> pre_token <span class="token keyword">in</span> regex<span class="token punctuation">.</span>finditer<span class="token punctuation">(</span>PAT<span class="token punctuation">,</span> pre_token_text<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="16"></td><td><pre>                word_bytes <span class="token operator">=</span> pre_token<span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="17"></td><td><pre>                words_counter<span class="token punctuation">[</span>split_bytes<span class="token punctuation">(</span>word_bytes<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span></pre></td></tr><tr><td data-num="18"></td><td><pre>                <span class="token keyword">del</span> word_bytes  <span class="token comment"># free memory</span></pre></td></tr><tr><td data-num="19"></td><td><pre>            last_end <span class="token operator">=</span> <span class="token keyword">match</span><span class="token punctuation">.</span>end<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="20"></td><td><pre></pre></td></tr><tr><td data-num="21"></td><td><pre>    <span class="token comment"># pretokenize after last special token</span></pre></td></tr><tr><td data-num="22"></td><td><pre>    pre_token_text <span class="token operator">=</span> chunk_str<span class="token punctuation">[</span>last_end<span class="token punctuation">:</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="23"></td><td><pre>    <span class="token keyword">for</span> pre_token <span class="token keyword">in</span> regex<span class="token punctuation">.</span>finditer<span class="token punctuation">(</span>PAT<span class="token punctuation">,</span> pre_token_text<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="24"></td><td><pre>        word_bytes <span class="token operator">=</span> pre_token<span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="25"></td><td><pre>        words_counter<span class="token punctuation">[</span>split_bytes<span class="token punctuation">(</span>word_bytes<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span></pre></td></tr><tr><td data-num="26"></td><td><pre>        <span class="token keyword">del</span> word_bytes</pre></td></tr><tr><td data-num="27"></td><td><pre></pre></td></tr><tr><td data-num="28"></td><td><pre>    <span class="token keyword">return</span> words_counter</pre></td></tr></tbody></table></figure><p>预分词时，需要将特殊 token 从语料中剔除，因为它们不会参与任何子词合并的过程，它们本身就是一个 token。匹配时首先找到特殊 token 的位置，然后对特殊 token 直接的文本进行预分词。</p>
<p>注意，在正则匹配过程中，应该使用  <code>finditer</code>  函数避免一次性完成所有匹配任务，导致内存开销过大。</p>
<h4 id="训练模块"><a class="anchor" href="#训练模块">#</a> 训练模块</h4>
<p>基于语料库的统计数据，从字节级别开始，迭代合并最高频、最大字典序的相邻字节对，逐步构建一个指定大小的词汇表。</p>
<p>训练代码如下：</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># train.py</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> pdb</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">import</span> pickle</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token keyword">from</span> collections <span class="token keyword">import</span> Counter<span class="token punctuation">,</span> defaultdict</pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token keyword">from</span> cs336_basics<span class="token punctuation">.</span>bpe<span class="token punctuation">.</span>pretokenize <span class="token keyword">import</span> pretokenize_file</pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token keyword">from</span> cs336_basics<span class="token punctuation">.</span>bpe<span class="token punctuation">.</span>utils <span class="token keyword">import</span> myHeap<span class="token punctuation">,</span> BytePair<span class="token punctuation">,</span> WordRef</pre></td></tr><tr><td data-num="9"></td><td><pre></pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token keyword">def</span> <span class="token function">train_bpe</span><span class="token punctuation">(</span></pre></td></tr><tr><td data-num="11"></td><td><pre>    input_path<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="12"></td><td><pre>    vocab_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="13"></td><td><pre>    special_tokens<span class="token punctuation">:</span> <span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="15"></td><td><pre>    <span class="token triple-quoted-string string">"""Train a BPE tokenizer on the given input file."""</span></pre></td></tr><tr><td data-num="16"></td><td><pre>    <span class="token comment"># 1. initialize byte-level vocab and add special tokens</span></pre></td></tr><tr><td data-num="17"></td><td><pre>    vocab <span class="token operator">=</span> <span class="token punctuation">{</span>i<span class="token punctuation">:</span> <span class="token builtin">bytes</span><span class="token punctuation">(</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">}</span>  <span class="token comment"># int to bytes</span></pre></td></tr><tr><td data-num="18"></td><td><pre>    <span class="token keyword">for</span> special_token <span class="token keyword">in</span> special_tokens <span class="token keyword">or</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="19"></td><td><pre>        vocab<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> special_token<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">,</span> errors<span class="token operator">=</span><span class="token string">"ignore"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="20"></td><td><pre></pre></td></tr><tr><td data-num="21"></td><td><pre>    <span class="token comment"># 2. pretokenize input file into words and count occurrences</span></pre></td></tr><tr><td data-num="22"></td><td><pre>    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>input_path<span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="23"></td><td><pre>        word_counts <span class="token operator">=</span> pretokenize_file<span class="token punctuation">(</span>f<span class="token punctuation">,</span> special_tokens<span class="token punctuation">)</span>  <span class="token comment"># Counter[Tuple[bytes, ...], int]</span></pre></td></tr><tr><td data-num="24"></td><td><pre></pre></td></tr><tr><td data-num="25"></td><td><pre>    <span class="token comment"># 3. create WordRef objects and initialize byte pair stats</span></pre></td></tr><tr><td data-num="26"></td><td><pre>    word_refs<span class="token punctuation">:</span> <span class="token builtin">list</span><span class="token punctuation">[</span>WordRef<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="27"></td><td><pre>    pair_to_count<span class="token punctuation">:</span> defaultdict<span class="token punctuation">[</span>BytePair<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="28"></td><td><pre>    pair_to_word<span class="token punctuation">:</span> defaultdict<span class="token punctuation">[</span>BytePair<span class="token punctuation">,</span> <span class="token builtin">set</span><span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="29"></td><td><pre>    <span class="token keyword">for</span> word_tuple<span class="token punctuation">,</span> count <span class="token keyword">in</span> word_counts<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="30"></td><td><pre>        word_ref <span class="token operator">=</span> WordRef<span class="token punctuation">(</span>tokens<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span>word_tuple<span class="token punctuation">)</span><span class="token punctuation">,</span> count<span class="token operator">=</span>count<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="31"></td><td><pre>        word_refs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>WordRef<span class="token punctuation">(</span>tokens<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span>word_tuple<span class="token punctuation">)</span><span class="token punctuation">,</span> count<span class="token operator">=</span>count<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="32"></td><td><pre>        <span class="token keyword">for</span> bp<span class="token punctuation">,</span> cnt <span class="token keyword">in</span> word_ref<span class="token punctuation">.</span>byte_pairs_dict<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="33"></td><td><pre>            pair_to_count<span class="token punctuation">[</span>bp<span class="token punctuation">]</span> <span class="token operator">+=</span> cnt</pre></td></tr><tr><td data-num="34"></td><td><pre>            pair_to_word<span class="token punctuation">[</span>bp<span class="token punctuation">]</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>word_refs<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="35"></td><td><pre></pre></td></tr><tr><td data-num="36"></td><td><pre>    pair_heap <span class="token operator">=</span> myHeap<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="37"></td><td><pre>    <span class="token keyword">for</span> bp<span class="token punctuation">,</span> cnt <span class="token keyword">in</span> pair_to_count<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="38"></td><td><pre>        pair_heap<span class="token punctuation">.</span>push<span class="token punctuation">(</span><span class="token punctuation">(</span>cnt<span class="token punctuation">,</span> bp<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="39"></td><td><pre>    pair_heap_to_delete <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># heap lazy deletion map</span></pre></td></tr><tr><td data-num="40"></td><td><pre></pre></td></tr><tr><td data-num="41"></td><td><pre>    <span class="token comment"># 4. merge BPEs until reaching the desired vocab size</span></pre></td></tr><tr><td data-num="42"></td><td><pre>    merges <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="43"></td><td><pre>    <span class="token keyword">with</span> tqdm<span class="token punctuation">(</span>total<span class="token operator">=</span>vocab_size <span class="token operator">-</span> <span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> desc<span class="token operator">=</span><span class="token string">"BPE Merging"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> pbar<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="44"></td><td><pre>        <span class="token keyword">while</span> <span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span> <span class="token operator">&lt;</span> vocab_size<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="45"></td><td><pre>            <span class="token comment"># Lazy deletion: pop until we find a valid top pair</span></pre></td></tr><tr><td data-num="46"></td><td><pre>            <span class="token keyword">while</span> pair_heap<span class="token punctuation">.</span>size <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="47"></td><td><pre>                top_cnt<span class="token punctuation">,</span> top_bp <span class="token operator">=</span> pair_heap<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="48"></td><td><pre>                <span class="token keyword">if</span> top_bp <span class="token keyword">in</span> pair_heap_to_delete<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="49"></td><td><pre>                    top_cnt <span class="token operator">-=</span> pair_heap_to_delete<span class="token punctuation">[</span>top_bp<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="50"></td><td><pre>                    <span class="token keyword">if</span> top_cnt <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="51"></td><td><pre>                        pair_heap<span class="token punctuation">.</span>push<span class="token punctuation">(</span><span class="token punctuation">(</span>top_cnt<span class="token punctuation">,</span> top_bp<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="52"></td><td><pre>                    <span class="token keyword">del</span> pair_heap_to_delete<span class="token punctuation">[</span>top_bp<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="53"></td><td><pre>                <span class="token keyword">else</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="54"></td><td><pre>                    <span class="token keyword">break</span></pre></td></tr><tr><td data-num="55"></td><td><pre>            <span class="token comment"># merge best pair</span></pre></td></tr><tr><td data-num="56"></td><td><pre>            <span class="token keyword">assert</span> top_bp <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token string">"No more pairs to merge"</span></pre></td></tr><tr><td data-num="57"></td><td><pre>            merges<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>top_bp<span class="token punctuation">.</span>left<span class="token punctuation">,</span> top_bp<span class="token punctuation">.</span>right<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="58"></td><td><pre>            new_token <span class="token operator">=</span> top_bp<span class="token punctuation">.</span>merged_bytes</pre></td></tr><tr><td data-num="59"></td><td><pre>            vocab<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> new_token</pre></td></tr><tr><td data-num="60"></td><td><pre></pre></td></tr><tr><td data-num="61"></td><td><pre>            <span class="token comment"># update word counts and pair stats</span></pre></td></tr><tr><td data-num="62"></td><td><pre>            pair_to_count_change<span class="token punctuation">:</span> defaultdict<span class="token punctuation">[</span>BytePair<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="63"></td><td><pre>            pair_heap_to_add <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span>  <span class="token comment"># cumulative count changes for lazy deletion</span></pre></td></tr><tr><td data-num="64"></td><td><pre>            <span class="token keyword">for</span> word_ref_id <span class="token keyword">in</span> pair_to_word<span class="token punctuation">[</span>top_bp<span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="65"></td><td><pre>                word_ref <span class="token operator">=</span> word_refs<span class="token punctuation">[</span>word_ref_id<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="66"></td><td><pre>                old_bp_dict <span class="token operator">=</span> word_ref<span class="token punctuation">.</span>byte_pairs_dict</pre></td></tr><tr><td data-num="67"></td><td><pre>                word_ref<span class="token punctuation">.</span>merge<span class="token punctuation">(</span>top_bp<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="68"></td><td><pre>                new_bp_dict <span class="token operator">=</span> word_ref<span class="token punctuation">.</span>byte_pairs_dict</pre></td></tr><tr><td data-num="69"></td><td><pre></pre></td></tr><tr><td data-num="70"></td><td><pre>                <span class="token comment"># update old pairs</span></pre></td></tr><tr><td data-num="71"></td><td><pre>                <span class="token keyword">for</span> bp<span class="token punctuation">,</span> cnt <span class="token keyword">in</span> old_bp_dict<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="72"></td><td><pre>                    <span class="token keyword">if</span> bp <span class="token keyword">not</span> <span class="token keyword">in</span> new_bp_dict <span class="token keyword">and</span> bp <span class="token operator">!=</span> top_bp<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="73"></td><td><pre>                        pair_to_word<span class="token punctuation">[</span>bp<span class="token punctuation">]</span><span class="token punctuation">.</span>remove<span class="token punctuation">(</span>word_ref_id<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="74"></td><td><pre>                    change <span class="token operator">=</span> new_bp_dict<span class="token punctuation">.</span>get<span class="token punctuation">(</span>bp<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">-</span> cnt</pre></td></tr><tr><td data-num="75"></td><td><pre>                    pair_to_count_change<span class="token punctuation">[</span>bp<span class="token punctuation">]</span> <span class="token operator">+=</span> change</pre></td></tr><tr><td data-num="76"></td><td><pre>                    <span class="token keyword">if</span> change <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="77"></td><td><pre>                        pair_heap_to_delete<span class="token punctuation">[</span>bp<span class="token punctuation">]</span> <span class="token operator">=</span> pair_heap_to_delete<span class="token punctuation">.</span>get<span class="token punctuation">(</span>bp<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">-</span> change</pre></td></tr><tr><td data-num="78"></td><td><pre></pre></td></tr><tr><td data-num="79"></td><td><pre>                <span class="token comment"># update new pairs</span></pre></td></tr><tr><td data-num="80"></td><td><pre>                <span class="token keyword">for</span> bp<span class="token punctuation">,</span> cnt <span class="token keyword">in</span> new_bp_dict<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="81"></td><td><pre>                    <span class="token keyword">if</span> bp <span class="token keyword">not</span> <span class="token keyword">in</span> old_bp_dict<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="82"></td><td><pre>                        pair_to_word<span class="token punctuation">[</span>bp<span class="token punctuation">]</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span>word_ref_id<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="83"></td><td><pre>                        pair_to_count_change<span class="token punctuation">[</span>bp<span class="token punctuation">]</span> <span class="token operator">+=</span> cnt</pre></td></tr><tr><td data-num="84"></td><td><pre>                        pair_heap_to_add<span class="token punctuation">[</span>bp<span class="token punctuation">]</span> <span class="token operator">+=</span> cnt</pre></td></tr><tr><td data-num="85"></td><td><pre></pre></td></tr><tr><td data-num="86"></td><td><pre>            <span class="token comment"># apply count changes</span></pre></td></tr><tr><td data-num="87"></td><td><pre>            <span class="token keyword">for</span> bp<span class="token punctuation">,</span> change <span class="token keyword">in</span> pair_to_count_change<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="88"></td><td><pre>                pair_to_count<span class="token punctuation">[</span>bp<span class="token punctuation">]</span> <span class="token operator">+=</span> change</pre></td></tr><tr><td data-num="89"></td><td><pre>                <span class="token keyword">if</span> pair_to_count<span class="token punctuation">[</span>bp<span class="token punctuation">]</span> <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="90"></td><td><pre>                    <span class="token keyword">del</span> pair_to_count<span class="token punctuation">[</span>bp<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="91"></td><td><pre>                    <span class="token keyword">del</span> pair_to_word<span class="token punctuation">[</span>bp<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="92"></td><td><pre></pre></td></tr><tr><td data-num="93"></td><td><pre>            <span class="token keyword">for</span> bp<span class="token punctuation">,</span> add_cnt <span class="token keyword">in</span> pair_heap_to_add<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="94"></td><td><pre>                pair_heap<span class="token punctuation">.</span>push<span class="token punctuation">(</span><span class="token punctuation">(</span>add_cnt<span class="token punctuation">,</span> bp<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># push updated count to heap</span></pre></td></tr><tr><td data-num="95"></td><td><pre>            <span class="token keyword">del</span> pair_heap_to_delete<span class="token punctuation">[</span>top_bp<span class="token punctuation">]</span>  <span class="token comment"># remove merged pair from lazy deletion map</span></pre></td></tr><tr><td data-num="96"></td><td><pre></pre></td></tr><tr><td data-num="97"></td><td><pre>            <span class="token keyword">assert</span> top_bp <span class="token keyword">not</span> <span class="token keyword">in</span> pair_to_count <span class="token keyword">and</span> top_bp <span class="token keyword">not</span> <span class="token keyword">in</span> pair_to_word</pre></td></tr><tr><td data-num="98"></td><td><pre></pre></td></tr><tr><td data-num="99"></td><td><pre>            <span class="token comment"># update progress bar</span></pre></td></tr><tr><td data-num="100"></td><td><pre>            pbar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="101"></td><td><pre></pre></td></tr><tr><td data-num="102"></td><td><pre>    <span class="token keyword">return</span> vocab<span class="token punctuation">,</span> merges</pre></td></tr></tbody></table></figure><p>整个函数流程基本符合第三节的内容，核心的三个数据是：</p>
<ul>
<li><code>word_refs</code> ：预分词得到的  <code>WordRef</code>  对象列表。</li>
<li><code>pair_to_count</code> ：相邻字节对的实时统计结果。</li>
<li><code>pair_to_word</code> ：相邻字节对出现的  <code>WordRef</code>  索引。</li>
</ul>
<p>每次合并时，从  <code>pair_to_word</code>  中取出受影响的  <code>WordRef</code>  对象，调用  <code>merge</code>  函数完成合并，并及时更新  <code>pair_to_count</code> 。</p>
<p>为了加速获取频率最高的字节对，这里使用了堆优化 + 延迟删除算法。合并时统计减少 / 删除的字节对，在下一次取最大值时进行延迟删除，并统计所有新增字节对更新堆。</p>
<h3 id="bpe-tokenizer"><a class="anchor" href="#bpe-tokenizer">#</a> BPE Tokenizer</h3>
<p>完成训练后，我们可以保存词表和合并规则序列，然后定义  <code>BPETokenizer</code>  类实现编码解码功能。</p>
<p>由于编码过程中也需要给字符串进行预分词，所以这里定义一个新的函数  <code>pretokenize_text</code> ，和前面预分词的逻辑类似，只是这里需要正确返回对应的特殊 token，因为它们也需要被编码成对应的 Token ID。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># pretokenize.py</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">def</span> <span class="token function">pretokenize_text</span><span class="token punctuation">(</span>text<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> special_tokens<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Iterable<span class="token punctuation">[</span><span class="token builtin">bytes</span><span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token triple-quoted-string string">"""</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    Pre-tokenize a string into words, replacing special tokens.</pre></td></tr><tr><td data-num="5"></td><td><pre>    Yields pre-tokenized byte sequences.</pre></td></tr><tr><td data-num="6"></td><td><pre>    """</pre></td></tr><tr><td data-num="7"></td><td><pre>    last_end <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment"># Track the end of the last match</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    <span class="token keyword">if</span> special_tokens <span class="token keyword">and</span> <span class="token builtin">len</span><span class="token punctuation">(</span>special_tokens<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="9"></td><td><pre>        special_tokens<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>key<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># Longer tokens first</span></pre></td></tr><tr><td data-num="10"></td><td><pre>        <span class="token keyword">for</span> <span class="token keyword">match</span> <span class="token keyword">in</span> re<span class="token punctuation">.</span>finditer<span class="token punctuation">(</span><span class="token string">"|"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>re<span class="token punctuation">.</span>escape<span class="token punctuation">(</span>st<span class="token punctuation">)</span> <span class="token keyword">for</span> st <span class="token keyword">in</span> special_tokens <span class="token keyword">or</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="11"></td><td><pre>            special_token <span class="token operator">=</span> <span class="token keyword">match</span><span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre>            <span class="token comment"># yield pre-tokens before the special token</span></pre></td></tr><tr><td data-num="14"></td><td><pre>            <span class="token keyword">for</span> pre_token <span class="token keyword">in</span> regex<span class="token punctuation">.</span>finditer<span class="token punctuation">(</span>PAT<span class="token punctuation">,</span> text<span class="token punctuation">[</span>last_end <span class="token punctuation">:</span> <span class="token keyword">match</span><span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="15"></td><td><pre>                <span class="token keyword">yield</span> pre_token<span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre></pre></td></tr><tr><td data-num="17"></td><td><pre>            <span class="token keyword">yield</span> special_token<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="18"></td><td><pre>            last_end <span class="token operator">=</span> <span class="token keyword">match</span><span class="token punctuation">.</span>end<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="19"></td><td><pre></pre></td></tr><tr><td data-num="20"></td><td><pre>    <span class="token keyword">for</span> pre_token <span class="token keyword">in</span> regex<span class="token punctuation">.</span>finditer<span class="token punctuation">(</span>PAT<span class="token punctuation">,</span> text<span class="token punctuation">[</span>last_end<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="21"></td><td><pre>        <span class="token keyword">yield</span> pre_token<span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><blockquote>
<p>这里将函数写成一个迭代器的形式，迭代返回一个字节串对象，这么做是为了兼容后面不同的编码需求。</p>
</blockquote>
<p>作业中， <code>BPETokenizer</code>  的数据结构如下：</p>
<ul>
<li><code>vocab: Dict[int, bytes]</code> ：Token ID 到字节串对象字典。</li>
<li><code>inv_vocab: Dict[bytes, int]</code> ： <code>vocab</code>  的反向字典。</li>
<li><code>merges: List[Tuple[bytes, bytes]]</code> ：合并规则序列。</li>
<li><code>special_tokens: List[str]</code> ：特殊 token 列表。</li>
<li><code>encode_cache: Dict[bytes, List[int]]</code> ：分词器编码的缓存结果，用于加速编码过程。</li>
<li><code>from_file</code> ：从文件中构建分词器对象。</li>
<li><code>_pretoken_to_ids</code> ：编码的核心逻辑，将一个预分词字节串转成对应的 Token ID，注意特殊 token 需要转换成其对应的 ID。</li>
<li><code>encode</code> ：编码一个给定字符串，返回编码后的整数序列。</li>
<li><code>encode_iterable</code> ：考虑到有时候需要编码的内容（比如文件）非常大，一次性读入内存不可取，因此传入字符串迭代器，返回编码后的整数 ID 迭代器，保证内存效率。</li>
<li><code>decode</code> ：将给定的整数序列解码成对应的字符串。由于整数序列由用户给定，有可能最终得到的字节串无法通过 UTF-8 进行解码，这里使用  <code>.decode("utf-8", errors="replace")</code>  将这些无效字节转换成标准的 Unicode 替代字符。</li>
</ul>
<p>废话少说，直接上代码实现：</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># tokenizer.py</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> pickle</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">import</span> time</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">from</span> typing <span class="token keyword">import</span> Dict<span class="token punctuation">,</span> Tuple<span class="token punctuation">,</span> List<span class="token punctuation">,</span> Union<span class="token punctuation">,</span> Iterable</pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token keyword">from</span> cs336_basics<span class="token punctuation">.</span>bpe<span class="token punctuation">.</span>pretokenize <span class="token keyword">import</span> pretokenize_text</pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token keyword">from</span> cs336_basics<span class="token punctuation">.</span>bpe<span class="token punctuation">.</span>utils <span class="token keyword">import</span> split_bytes<span class="token punctuation">,</span> WordRef<span class="token punctuation">,</span> BytePair</pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token keyword">class</span> <span class="token class-name">Tokenizer</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    vocab<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">,</span> <span class="token builtin">bytes</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="11"></td><td><pre>    inv_vocab<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span><span class="token builtin">bytes</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="12"></td><td><pre>    merges<span class="token punctuation">:</span> List<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span><span class="token builtin">bytes</span><span class="token punctuation">,</span> <span class="token builtin">bytes</span><span class="token punctuation">]</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="13"></td><td><pre>    special_tokens<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="14"></td><td><pre>    encode_cache<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span><span class="token builtin">bytes</span><span class="token punctuation">,</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span></pre></td></tr><tr><td data-num="15"></td><td><pre></pre></td></tr><tr><td data-num="16"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">,</span> <span class="token builtin">bytes</span><span class="token punctuation">]</span><span class="token punctuation">,</span> merges<span class="token punctuation">:</span> List<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span><span class="token builtin">bytes</span><span class="token punctuation">,</span> <span class="token builtin">bytes</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> special_tokens<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="17"></td><td><pre>        self<span class="token punctuation">.</span>vocab <span class="token operator">=</span> vocab</pre></td></tr><tr><td data-num="18"></td><td><pre>        self<span class="token punctuation">.</span>inv_vocab <span class="token operator">=</span> <span class="token punctuation">{</span>v<span class="token punctuation">:</span> k <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> vocab<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></pre></td></tr><tr><td data-num="19"></td><td><pre>        self<span class="token punctuation">.</span>merges <span class="token operator">=</span> merges</pre></td></tr><tr><td data-num="20"></td><td><pre>        self<span class="token punctuation">.</span>special_tokens <span class="token operator">=</span> special_tokens <span class="token keyword">or</span> <span class="token punctuation">[</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="21"></td><td><pre></pre></td></tr><tr><td data-num="22"></td><td><pre>    <span class="token decorator annotation punctuation">@classmethod</span></pre></td></tr><tr><td data-num="23"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">from_file</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> vocab_filepath<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> merges_filepath<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> special_tokens<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token operator">|</span> <span class="token boolean">None</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="24"></td><td><pre>        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>vocab_filepath<span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> vf<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="25"></td><td><pre>            vocab <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>vf<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="26"></td><td><pre>        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>merges_filepath<span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> mf<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="27"></td><td><pre>            merges <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>mf<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="28"></td><td><pre>        <span class="token keyword">return</span> cls<span class="token punctuation">(</span>vocab<span class="token operator">=</span>vocab<span class="token punctuation">,</span> merges<span class="token operator">=</span>merges<span class="token punctuation">,</span> special_tokens<span class="token operator">=</span>special_tokens<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="29"></td><td><pre></pre></td></tr><tr><td data-num="30"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">_pretoken_to_ids</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">:</span> <span class="token builtin">bytes</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Iterable<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="31"></td><td><pre>        <span class="token triple-quoted-string string">"""Encode a byte string into a list of token IDs using the BPE merges"""</span></pre></td></tr><tr><td data-num="32"></td><td><pre>        <span class="token keyword">if</span> text <span class="token keyword">in</span> self<span class="token punctuation">.</span>encode_cache<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="33"></td><td><pre>            <span class="token keyword">yield</span> <span class="token keyword">from</span> self<span class="token punctuation">.</span>encode_cache<span class="token punctuation">[</span>text<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="34"></td><td><pre>            <span class="token keyword">return</span></pre></td></tr><tr><td data-num="35"></td><td><pre></pre></td></tr><tr><td data-num="36"></td><td><pre>        word_ref <span class="token operator">=</span> WordRef<span class="token punctuation">(</span>tokens<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span>split_bytes<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> count<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="37"></td><td><pre>        byte_pairs_dict <span class="token operator">=</span> word_ref<span class="token punctuation">.</span>byte_pairs_dict</pre></td></tr><tr><td data-num="38"></td><td><pre>        <span class="token keyword">for</span> merge_left<span class="token punctuation">,</span> merge_right <span class="token keyword">in</span> self<span class="token punctuation">.</span>merges<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="39"></td><td><pre>            bp <span class="token operator">=</span> BytePair<span class="token punctuation">(</span>merge_left<span class="token punctuation">,</span> merge_right<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="40"></td><td><pre>            <span class="token keyword">if</span> bp <span class="token keyword">not</span> <span class="token keyword">in</span> byte_pairs_dict<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="41"></td><td><pre>                <span class="token keyword">continue</span></pre></td></tr><tr><td data-num="42"></td><td><pre>            word_ref<span class="token punctuation">.</span>merge<span class="token punctuation">(</span>bp<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="43"></td><td><pre>            byte_pairs_dict <span class="token operator">=</span> word_ref<span class="token punctuation">.</span>byte_pairs_dict</pre></td></tr><tr><td data-num="44"></td><td><pre>            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>word_ref<span class="token punctuation">.</span>tokens<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="45"></td><td><pre>                <span class="token keyword">break</span></pre></td></tr><tr><td data-num="46"></td><td><pre></pre></td></tr><tr><td data-num="47"></td><td><pre>        token_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="48"></td><td><pre>        <span class="token keyword">for</span> token <span class="token keyword">in</span> word_ref<span class="token punctuation">.</span>tokens<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="49"></td><td><pre>            <span class="token keyword">yield</span> self<span class="token punctuation">.</span>inv_vocab<span class="token punctuation">[</span>token<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="50"></td><td><pre>            token_ids<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>inv_vocab<span class="token punctuation">[</span>token<span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="51"></td><td><pre></pre></td></tr><tr><td data-num="52"></td><td><pre>        self<span class="token punctuation">.</span>encode_cache<span class="token punctuation">[</span>text<span class="token punctuation">]</span> <span class="token operator">=</span> token_ids</pre></td></tr><tr><td data-num="53"></td><td><pre></pre></td></tr><tr><td data-num="54"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">encode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="55"></td><td><pre>        <span class="token triple-quoted-string string">"""Encode a string into a list of token IDs using the BPE merges"""</span></pre></td></tr><tr><td data-num="56"></td><td><pre>        encoded_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="57"></td><td><pre>        <span class="token comment"># reset cache with special tokens</span></pre></td></tr><tr><td data-num="58"></td><td><pre>        self<span class="token punctuation">.</span>encode_cache <span class="token operator">=</span> <span class="token punctuation">{</span>st<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>inv_vocab<span class="token punctuation">[</span>st<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> st <span class="token keyword">in</span> self<span class="token punctuation">.</span>special_tokens<span class="token punctuation">}</span></pre></td></tr><tr><td data-num="59"></td><td><pre>        <span class="token keyword">for</span> pre_toeken <span class="token keyword">in</span> pretokenize_text<span class="token punctuation">(</span>text<span class="token punctuation">,</span> self<span class="token punctuation">.</span>special_tokens<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="60"></td><td><pre>            <span class="token keyword">for</span> token_id <span class="token keyword">in</span> self<span class="token punctuation">.</span>_pretoken_to_ids<span class="token punctuation">(</span>pre_toeken<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="61"></td><td><pre>                encoded_ids<span class="token punctuation">.</span>append<span class="token punctuation">(</span>token_id<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="62"></td><td><pre></pre></td></tr><tr><td data-num="63"></td><td><pre>        <span class="token keyword">return</span> encoded_ids</pre></td></tr><tr><td data-num="64"></td><td><pre></pre></td></tr><tr><td data-num="65"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">encode_iterable</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> iterable<span class="token punctuation">:</span> Iterable<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Iterable<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="66"></td><td><pre>        <span class="token triple-quoted-string string">"""Encode an iterable of strings into a flat iterable of token IDs using the BPE merges"""</span></pre></td></tr><tr><td data-num="67"></td><td><pre>        self<span class="token punctuation">.</span>encode_cache <span class="token operator">=</span> <span class="token punctuation">{</span>st<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>inv_vocab<span class="token punctuation">[</span>st<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> st <span class="token keyword">in</span> self<span class="token punctuation">.</span>special_tokens<span class="token punctuation">}</span></pre></td></tr><tr><td data-num="68"></td><td><pre>        <span class="token keyword">for</span> text <span class="token keyword">in</span> iterable<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="69"></td><td><pre>            <span class="token keyword">for</span> pre_toeken <span class="token keyword">in</span> pretokenize_text<span class="token punctuation">(</span>text<span class="token punctuation">,</span> self<span class="token punctuation">.</span>special_tokens<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="70"></td><td><pre>                <span class="token keyword">for</span> token_id <span class="token keyword">in</span> self<span class="token punctuation">.</span>_pretoken_to_ids<span class="token punctuation">(</span>pre_toeken<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="71"></td><td><pre>                    <span class="token keyword">yield</span> token_id</pre></td></tr><tr><td data-num="72"></td><td><pre></pre></td></tr><tr><td data-num="73"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">decode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> ids<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">str</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="74"></td><td><pre>        bytes_seq <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="75"></td><td><pre>        <span class="token keyword">for</span> i <span class="token keyword">in</span> ids<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="76"></td><td><pre>            <span class="token keyword">if</span> i <span class="token operator">&gt;=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="77"></td><td><pre>                Warning<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Token ID </span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string"> not in vocab, replacing with b''"</span></span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="78"></td><td><pre>            bytes_seq<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>get<span class="token punctuation">(</span>i<span class="token punctuation">,</span> <span class="token string">b""</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="79"></td><td><pre>        seq <span class="token operator">=</span> <span class="token string">b""</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>self<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>get<span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> ids<span class="token punctuation">)</span><span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">,</span> errors<span class="token operator">=</span><span class="token string">"replace"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="80"></td><td><pre>        <span class="token keyword">return</span> seq</pre></td></tr></tbody></table></figure></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-eye"></i></span><span class="text">总访问量：</span><span class="waline-pageview-count" id="twikoo_visitors" data-path="/stanford-cs336/bpe-tokenizer-algorithm/">加载中...</span></span><span class="item"><span class="icon"><i class="ic i-calendar-check"></i></span><span class="text">更新于 </span><time title="修改时间：2025-11-28 22:35:09" itemprop="dateModified" datetime="2025-11-28T22:35:09+08:00">2025-11-28</time></span></div><div id="copyright"><ul><li class="author"><strong>本文作者：</strong>Jongsh<i class="ic i-at"><em>@</em></i>穷拾の小屋</li><li class="link"><strong>本文链接：</strong><a href="https://blog.jongsh.top/stanford-cs336/bpe-tokenizer-algorithm/" title="BPE 分词器算法原理与实现">https://blog.jongsh.top/stanford-cs336/bpe-tokenizer-algorithm/</a></li><li class="license"><strong>版权声明：</strong>本站所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/coding-diary/pytorch-note/pytorch-model-training-parallel/" rel="prev" itemprop="url" data-background-image="https://jongsh.oss-cn-beijing.aliyuncs.com/blog/cover/202510121638032.png?x-oss-process=image/format,webp" title="PyTorch 模型训练之并行篇"><span class="type">上一篇</span><span class="category"><i class="ic i-flag"></i>PyTorch 学习笔记</span><h3>PyTorch 模型训练之并行篇</h3></a></div><div class="item right"></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text"> 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#unicode-%E7%BC%96%E7%A0%81%E6%A0%87%E5%87%86"><span class="toc-number">2.</span> <span class="toc-text"> Unicode 编码标准</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bpe-tokenizer-%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text"> BPE Tokenizer 训练算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E8%A1%A8%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">3.1.</span> <span class="toc-text"> 词表初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E5%88%86%E8%AF%8D"><span class="toc-number">3.2.</span> <span class="toc-text"> 预分词</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%90%E8%AF%8D%E5%90%88%E5%B9%B6"><span class="toc-number">3.3.</span> <span class="toc-text"> 子词合并</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bpe-tokenizer-%E7%BC%96%E7%A0%81%E8%A7%A3%E7%A0%81"><span class="toc-number">4.</span> <span class="toc-text"> BPE Tokenizer 编码解码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.</span> <span class="toc-text"> 代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AD%E6%96%99"><span class="toc-number">5.1.</span> <span class="toc-text"> 训练语料</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"><span class="toc-number">5.2.</span> <span class="toc-text"> 目录结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bpe-%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0"><span class="toc-number">5.3.</span> <span class="toc-text"> BPE 训练函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B7%A5%E5%85%B7%E6%A8%A1%E5%9D%97"><span class="toc-number">5.3.1.</span> <span class="toc-text"> 工具模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9D%97"><span class="toc-number">5.3.2.</span> <span class="toc-text"> 预分词模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9D%97"><span class="toc-number">5.3.3.</span> <span class="toc-text"> 训练模块</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bpe-tokenizer"><span class="toc-number">5.4.</span> <span class="toc-text"> BPE Tokenizer</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li class="active"><a href="/stanford-cs336/bpe-tokenizer-algorithm/" rel="bookmark" title="BPE 分词器算法原理与实现">BPE 分词器算法原理与实现</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><img class="image lozaded" loading="lazy" decoding="async" itemprop="image" alt="Jongsh" src="/assets/avatar.avif"><p class="name" itemprop="name">Jongsh</p><div class="description" itemprop="description">Talk is cheap, show me the code!</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">12</span><span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">8</span><span class="name">分类</span></a></div></nav><div class="social"><a target="_blank" rel="noopener" href="https://github.com/jongsh" class="item github" title="https://github.com/jongsh"><i class="ic i-github"></i></a><a target="_blank" rel="noopener" href="https://gitee.com/jongsh" class="item gitee" title="https://gitee.com/jongsh"><i class="ic i-gitee"></i></a><a href="/cjh1967662798@outlook.com" class="item envelope" title="cjh1967662798@outlook.com"><i class="ic i-envelope"></i></a></div><div class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="#" onclick="return false;"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友链</a></li></div></div></div></div><ul id="quick"><li class="prev pjax"></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/coding-diary/pytorch-note/pytorch-model-training-parallel/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div><div id="player"></div></main></div><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/coding-diary/" title="分类于编程日常">编程日常</a><i class="ic i-angle-right"></i><a href="/categories/coding-diary/pytorch-note/" title="分类于PyTorch 学习笔记">PyTorch 学习笔记</a></div><span><a href="/coding-diary/pytorch-note/pytorch-model-training-basics/">PyTorch 模型训练之基础篇</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/blog-site/" title="分类于博客网站">博客网站</a></div><span><a href="/blog-site/cabin-building-record/">小屋搭建记录</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/learning-note/" title="分类于学习笔记">学习笔记</a><i class="ic i-angle-right"></i><a href="/categories/learning-note/frontend-development/" title="分类于前端开发">前端开发</a></div><span><a href="/learning-note/frontend-development/vue3-project-build-guide/">Vue3 项目构建指南</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/coding-diary/" title="分类于编程日常">编程日常</a><i class="ic i-angle-right"></i><a href="/categories/coding-diary/pytorch-note/" title="分类于PyTorch 学习笔记">PyTorch 学习笔记</a></div><span><a href="/coding-diary/pytorch-note/pytorch-model-training-parallel/">PyTorch 模型训练之并行篇</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/learning-note/" title="分类于学习笔记">学习笔记</a><i class="ic i-angle-right"></i><a href="/categories/learning-note/server-configuration/" title="分类于服务器配置">服务器配置</a></div><span><a href="/learning-note/server-configuration/linux-nfs-cifs/">Linux 环境配置 NFS 与 CIFS</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/blog-site/" title="分类于博客网站">博客网站</a></div><span><a href="/blog-site/hello-world/">世界，你好</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/coding-diary/" title="分类于编程日常">编程日常</a></div><span><a href="/coding-diary/python-multiprocessing/">Python 多进程编程</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/learning-note/" title="分类于学习笔记">学习笔记</a><i class="ic i-angle-right"></i><a href="/categories/learning-note/reinforcement-learning/" title="分类于强化学习">强化学习</a></div><span><a href="/learning-note/reinforcement-learning/reinforcement-learning-math/">强化学习的数学原理</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/coding-diary/" title="分类于编程日常">编程日常</a></div><span><a href="/coding-diary/weibo-poi-crawler/">微博 POI 数据爬取</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/announcement/">网站迁移完成</a></span></li></ul></div><div class="rpost pjax"><h2>最新评论</h2><ul class="leancloud-recent-comment" id="new-comment"></ul></div></div><div class="status"><div class="copyright">© 2024 -<span itemprop="copyrightYear">2026</span><span class="with-love"><i class="ic i-sakura rotate"></i></span><span class="author" itemprop="copyrightHolder">Jongsh @ JONGSH'S BLOG</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i></span><span title="站点总字数">154k 字</span><span class="post-meta-divider"> | </span><span class="post-meta-item-icon"><i class="ic i-coffee"></i></span><span title="站点阅读时长">2:20</span></div><div class="powered-by">基于 <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> &amp; Theme.<a target="_blank" rel="noopener" href="https://github.com/theme-shoka-x/hexo-theme-shokaX/">ShokaX</a></div></div></div></footer></div><script data-config="" type="text/javascript">var LOCAL = {
    ispost: true,
    path: `stanford-cs336/bpe-tokenizer-algorithm/`,
    favicon: {
        show: `(๑◔‿◔๑)被我骗了吧~`,
        hide: `(×_×) 404网站未响应`
    },
    search: {
        placeholder: "文章搜索",
        empty: "关于 「 ${query} 」，什么也没搜到",
        stats: "${time} ms 内找到 ${hits} 条结果"
    },
    nocopy: "false",
    copyright: `复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。`,
    copy_tex: true,
    katex: true,
    mermaid: false,
    audio: undefined,
    fancybox: true,
    nocopy: false,
    outime: true,
    template: `<div class="note warning"><p><span class="label warning">文章时效性提示</span><br>这是一篇发布于 {{publish}} 天前，最后一次更新在 {{updated}} 天前的文章，部分信息可能已经发生改变，请注意甄别。</p></div>`,
    quiz: {
        choice: `单选题`,
        multiple: `多选题`,
        true_false: `判断题`,
        essay: `问答题`,
        gap_fill: `填空题`,
        mistake: `错题备注`
    },
    ignores: [
        (uri) => uri.includes('#'),
        (uri) => new RegExp(LOCAL.path + '$').test(uri),
            []
    ]
};
</script><script src="https://s4.zstatic.net/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha384-k6YtvFUEIuEFBdrLKJ3YAUbBki333tj1CSUisai5Cswsg9wcLNaPzsTHDswp4Az8" crossorigin="anonymous" fetchpriority="high"></script><script src="https://s4.zstatic.net/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous" fetchpriority="high"></script><script src="https://s4.zstatic.net/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha384-Zm+UU4tdcfAm29vg+MTbfu//q5B/lInMbMCr4T8c9rQFyOv6PlfQYpB5wItcXWe7" crossorigin="anonymous" fetchpriority="high"></script><script src="https://s4.zstatic.net/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" integrity="sha384-TOxsBplaL96/QDWPIUg+ye3v89qSE3s22XNtJMmCeZEep3cVDmXy1zEfZvVv+y2m" crossorigin="anonymous" fetchpriority="high"></script><script src="/js/siteInit.js?v=0.4.25" type="module" fetchpriority="high" defer=""></script></body></html>
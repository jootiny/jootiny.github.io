<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="X-UA-COMPATIBLE" content="IE=edge,chrome=1"><meta name="renderer" content="webkit"><link rel="icon" type="image/ico" sizes="32x32" href="/assets/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png"><link rel="alternate" href="/rss.xml" title="穷拾の小屋" type="application/rss+xml"><link rel="alternate" href="/atom.xml" title="穷拾の小屋" type="application/atom+xml"><link rel="alternate" type="application/json" title="穷拾の小屋" href="https://blog.jongsh.top/feed.json"><link rel="preconnect" href="https://s4.zstatic.net"><link rel="preconnect" href="https://at.alicdn.com"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7CFredericka%20the%20Great:400,400italic,700,700italic%7CNoto%20Serif%20JP:400,400italic,700,700italic%7CNoto%20Serif%20SC:400,400italic,700,700italic%7CInconsolata:400,400italic,700,700italic&amp;display=swap&amp;subset=latin,latin-ext" media="none" onload="this.media='all'"><link rel="stylesheet" href="/css/app.css?v=0.4.25"><link rel="modulepreload" href="/js/chunk-3DLRVTZI.js"><link rel="modulepreload" href="/js/chunk-G6WST2TT.js"><link rel="modulepreload" href="/js/chunk-MEQLBSIV.js"><link rel="modulepreload" href="/js/chunk-T4ZN27XU.js"><link rel="modulepreload" href="/js/chunk-WIQECBEN.js"><link rel="modulepreload" href="/js/comments-RAU2KDSH.js"><link rel="modulepreload" href="/js/copy-tex-JHKIWPSS.js"><link rel="modulepreload" href="/js/post-DEG2LM3E.js"><link rel="modulepreload" href="/js/quicklink-VPIEOOQW.js"><link rel="modulepreload" href="/js/search-7R35WZ2Q.js"><link rel="modulepreload" href="/js/siteInit.js"><link rel="modulepreload" href="/js/waline-MSCX2FFD.js"><link rel="stylesheet" href="/css/comments-F4ZGS7LD.css" media="none" onload="this.media='all'"><link rel="stylesheet" href="/css/siteInit.css" media="none" onload="this.media='all'"><link rel="stylesheet" href="/css/waline-IDNZKML2.css" media="none" onload="this.media='all'"><link rel="preload" href="https://jongsh.oss-cn-beijing.aliyuncs.com/blog/cover/202510121638032.png?x-oss-process=image/format,webp" as="image" fetchpriority="high"><meta name="keywords" content="穷拾, jongsh, 博客, blog"><meta name="description" content="Talk is cheap, show me the code!"><link rel="canonical" href="https://blog.jongsh.top/coding-diary/pytorch-note/pytorch-model-training-parallel/"><title>PyTorch 模型训练之并行篇</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope="" itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">PyTorch 模型训练之并行篇</h1><div class="meta"><span class="item" title="创建时间：2025-09-22 20:15:12"><span class="icon"><i class="ic i-calendar"></i></span><span class="text">发表于</span><time itemprop="dateCreated datePublished" datetime="2025-09-22T20:15:12+08:00">2025-09-22</time></span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i></span><span class="text">本文字数</span><span>18k</span><span class="text">字</span></span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i></span><span class="text">阅读时长</span><span>16 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span><span class="line"></span><span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">JONGSH'S BLOG</a></li></ul><ul class="right" id="rightNav"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div class="pjax" id="imgs"><img src="https://jongsh.oss-cn-beijing.aliyuncs.com/blog/cover/202510121638032.png?x-oss-process=image/format,webp" loading="eager" decoding="async" fetchpriority="high" alt="穷拾の小屋"></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"></path></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"></use><use xlink:href="#gentle-wave" x="48" y="3"></use><use xlink:href="#gentle-wave" x="48" y="5"></use><use xlink:href="#gentle-wave" x="48" y="7"></use></g></svg></div><div id="main-background" style="background-image: url(https://jongsh.oss-cn-beijing.aliyuncs.com/blog/others/202409152212632.png?x-oss-process=image/format,webp); position: relative;">   <main><div class="inner"><div class="pjax" id="main"><div class="article wrap"><div class="breadcrumb" itemlistelement="" itemscope="" itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i><span><a href="/">首页</a></span><i class="ic i-angle-right"></i><span itemprop="itemListElement" itemscope="itemscope" itemtype="https://schema.org/ListItem"><a href="/categories/coding-diary/" itemprop="item" rel="index" title="分类于编程日常"><span itemprop="name">编程日常<meta itemprop="position" content="0"></span></a></span><i class="ic i-angle-right"></i><span class="current" itemprop="itemListElement" itemscope="itemscope" itemtype="https://schema.org/ListItem"><a href="/categories/coding-diary/pytorch-note/" itemprop="item" rel="index" title="分类于PyTorch 学习笔记"><span itemprop="name">PyTorch 学习笔记<meta itemprop="position" content="1"></span></a></span></div><article class="post block" itemscope="itemscope" itemtype="http://schema.org/Article" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://blog.jongsh.top/coding-diary/pytorch-note/pytorch-model-training-parallel/"><span hidden="hidden" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><meta itemprop="image" content="/assets/avatar.jpg"><meta itemprop="name" content="Jongsh"><meta itemprop="description" content=", Talk is cheap, show me the code!"></span><span hidden="hidden" itemprop="publisher" itemscope="itemscope" itemtype="http://schema.org/Organization"><meta itemprop="name" content="穷拾の小屋"></span><div class="body md" itemprop="articleBody"><p>现如今，深度学习模型的规模和复杂度正以前所未有的速度增长，从简单的线性模型到如今动辄数百数千亿参数的大模型，单 GPU 的计算能力已难以满足高效训练的需求。并行训练技术应运而生，它合理分配计算任务到多个 GPU 或多台机器上，显著提升了训练效率。本文将介绍 PyTorch 中几种常见的并行训练方法，包括数据并行（Data Parallel）、分布式数据并行（Distributed Data Parallel，DDP）、Hugging Face 的 Accelerate 工具库等。通过对比和代码示例，帮助读者快速掌握这些技术的原理和使用方法。</p>
<h2 id="前期准备"><a class="anchor" href="#前期准备">#</a> 前期准备</h2>
<p>为了演示不同的并行训练方案，首先需要准备对应的训练数据和模型。由于现在不需要关注模型的准确率等指标，因此我们可以使用随机数据来进行训练。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># data.py</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> torch</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre>n <span class="token operator">=</span> <span class="token number">5000</span></pre></td></tr><tr><td data-num="6"></td><td><pre>feature <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>label <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>n<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>feature<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> label<span class="token punctuation">.</span>shape<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>feature<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre>np<span class="token punctuation">.</span>savez<span class="token punctuation">(</span><span class="token string">"data.npz"</span><span class="token punctuation">,</span> feature<span class="token operator">=</span>feature<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token operator">=</span>label<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p>对于模型，使用简单的 ResNet18 即可。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># train.py</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> os</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">import</span> torch</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">import</span> time</pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> models</pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token keyword">import</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">as</span> dist</pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn</pre></td></tr><tr><td data-num="10"></td><td><pre></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel <span class="token keyword">import</span> DistributedDataParallel <span class="token keyword">as</span> DDP</pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre><span class="token keyword">def</span> <span class="token function">get_model</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="14"></td><td><pre>    model <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>weights<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="15"></td><td><pre>    model<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre>    model<span class="token punctuation">.</span>fc <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="17"></td><td><pre>    <span class="token keyword">return</span> model</pre></td></tr></tbody></table></figure><p>至此，数据和模型都准备好了，接下来我们就可以开始介绍不同的并行训练方案了。</p>
<br> 
<h2 id="data-parallel"><a class="anchor" href="#data-parallel">#</a> Data Parallel</h2>
<h3 id="原理介绍"><a class="anchor" href="#原理介绍">#</a> 原理介绍</h3>
<p>Data Parallel（DP）数据并行是一种常见的分布式训练方法。<strong>在 DP 中，模型的副本会被复制到每个 GPU 上，每一批次的数据会被划分成多个子批次，分别分配到各个 GPU 上进行前向传播，反向传播计算出对应的梯度。然后，这些梯度会被汇总并应用到主模型上，从而更新模型的参数，然后再将更新后的参数广播到各个 GPU 上</strong>。</p>
<p>在这里，DP 仅会创建一个进程，所以会受到 Python 全局解释器锁（GIL）的影响。不仅如此，由于所有参数的梯度需要汇总到主模型上，每次  <code>optimizer.step()</code>  都需要进行通信操作，因此 DP 的并行训练效率较低。</p>
<blockquote>
<p>参考资料：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/675217571">深入理解 PyTorch 数据并行模块 DataParallel 及其反向传播细节 - 知乎</a></p>
</blockquote>
<h3 id="代码实现"><a class="anchor" href="#代码实现">#</a> 代码实现</h3>
<p>DP 的训练方案实现最为简单，只需要在原有模型的基础上，使用  <code>torch.nn.DataParallel</code>  包装一下即可。接下来逐步介绍 DP 的实现过程。</p>
<p>首先定义数据加载器，这和之前的单 GPU 训练是一样的，其中的  <code>batch_size</code>  指的是每次从数据集中取出的样本数量，而不是每个 GPU 上的样本数量。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># train.py</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">def</span> <span class="token function">get_dp_dataset_loader</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    data <span class="token operator">=</span> np<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"data.npz"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    feature <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">"feature"</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    label <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    feature <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>feature<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    label <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>label<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    dataset <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>feature<span class="token punctuation">,</span> label<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    data_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    <span class="token keyword">return</span> data_loader</pre></td></tr></tbody></table></figure><p>然后定义训练函数  <code>train_dp</code> ，使用 DP 训练模型：</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># train.py</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">def</span> <span class="token function">train_dp</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    device_ids <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 默认使用所有 GPU</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    model <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device_ids<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 将模型放到主 GPU 上</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    model <span class="token operator">=</span> nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span>device_ids<span class="token punctuation">)</span>  <span class="token comment"># 这里使用 DataParallel 包装模型</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    data_loader <span class="token operator">=</span> get_dp_dataset_loader<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre></pre></td></tr><tr><td data-num="8"></td><td><pre>    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    loss_fn <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre></pre></td></tr><tr><td data-num="11"></td><td><pre>    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="13"></td><td><pre>        epoch_loss <span class="token operator">=</span> <span class="token number">0.0</span></pre></td></tr><tr><td data-num="14"></td><td><pre>        num_batches <span class="token operator">=</span> <span class="token number">0</span></pre></td></tr><tr><td data-num="15"></td><td><pre>        <span class="token keyword">for</span> _<span class="token punctuation">,</span> <span class="token punctuation">(</span>feature<span class="token punctuation">,</span> label<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="16"></td><td><pre>            feature <span class="token operator">=</span> feature<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device_ids<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> non_blocking<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># 将数据放到主 GPU 上</span></pre></td></tr><tr><td data-num="17"></td><td><pre>            label <span class="token operator">=</span> label<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device_ids<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> non_blocking<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># 将数据放到主 GPU 上</span></pre></td></tr><tr><td data-num="18"></td><td><pre></pre></td></tr><tr><td data-num="19"></td><td><pre>            <span class="token comment"># 代码不变</span></pre></td></tr><tr><td data-num="20"></td><td><pre>            output <span class="token operator">=</span> model<span class="token punctuation">(</span>feature<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="21"></td><td><pre>            loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> label<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="22"></td><td><pre>            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="23"></td><td><pre>            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="24"></td><td><pre>            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="25"></td><td><pre></pre></td></tr><tr><td data-num="26"></td><td><pre>            epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="27"></td><td><pre>            num_batches <span class="token operator">+=</span> <span class="token number">1</span></pre></td></tr><tr><td data-num="28"></td><td><pre></pre></td></tr><tr><td data-num="29"></td><td><pre>        epoch_loss <span class="token operator">/=</span> num_batches</pre></td></tr><tr><td data-num="30"></td><td><pre>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Epoch </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token punctuation">}</span></span><span class="token string"> - Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_loss<span class="token punctuation">:</span><span class="token format-spec">.6f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="31"></td><td><pre></pre></td></tr><tr><td data-num="32"></td><td><pre><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="33"></td><td><pre>    train_dp<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><blockquote>
<p>这里为了简代码，默认使用所有可用的 GPU 进行训练。</p>
</blockquote>
<p>执行以下命令运行训练脚本：</p>
<figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>python train.py</pre></td></tr></tbody></table></figure><p>DP 使用  <code>torch.nn.DataParallel</code>  包装模型。 <code>torch.nn.DataParallel</code>  包装后的模型内部实现了多 GPU 的数据拆分、模型复制和梯度聚合，对外的接口不变。另外，在模型初始化、数据初始化时都需要先放在主 GPU（默认是第一个 GPU）上，然后由主 GPU 负责复制模型，拆分数据并分发到各个 GPU 上进行计算，最后再将各个 GPU 上计算得到的梯度汇总到主 GPU 上进行参数更新。</p>
<p>因此，只需几行代码即可将单 GPU 训练代码改为多 GPU 训练代码。<strong>但是，DP 训练方式中，主 GPU 不仅要参与训练，还负责数据分发和梯度收集，低效的通信方式成为性能瓶颈，往往导致多卡利用率低。实测下来，DP 多卡训练甚至效率不如单卡训练，因此不推荐使用。</strong></p>
<br> 
<h2 id="distributed-data-parallel"><a class="anchor" href="#distributed-data-parallel">#</a> Distributed Data Parallel</h2>
<h3 id="原理介绍-2"><a class="anchor" href="#原理介绍-2">#</a> 原理介绍</h3>
<p>Distributed Data Parallel（DDP）并行化训练模型的原理类似于 DP，都是将一个批次的数据拆分到多个 GPU 上并发训练，最后汇总梯度更新模型，但采用了更加高效的实现方法：</p>
<ol>
<li>在 DDP 模式下，有 N 个进程被启动，每个进程在一张卡上加载一个模型，这些模型的参数在数值上是相同的。</li>
<li>在模型训练时，各个进程通过一种叫 Ring-Reduce 的方法与其他进程通讯，交换各自的梯度，提高了通讯效率。</li>
<li>各个进程用平均后的梯度更新自己的参数，因为各个进程的初始参数、更新梯度是一致的，所以更新后的参数也是完全相同的。</li>
</ol>
<blockquote>
<p>一般来说，DDP 都是显著地比 DP 快，能达到略低于卡数的加速比（例如，四卡下加速 3 倍）。所以，其是目前最流行的多机多卡训练方法。</p>
</blockquote>
<p>更精确来说，PyTorch 中使用 DDP 有如下三种情况：</p>
<ul>
<li>每个进程一张卡。这是 DDP 的最佳使用方法。</li>
<li>每个进程多张卡，复制模式。一个模型复制在不同卡上面，每个进程都实质等同于 DP 模式。速度不如第一种方法，一般不采用。</li>
<li>每个进程多张卡，并行模式。一个模型的不同部分分布在不同的卡上面。这种场景，一般是因为我们的模型非常大，大到一张卡都塞不下一个模型。</li>
</ul>
<p>在这里，我们仅讨论第一种情况。原因是，第二种情况完全没有必要，效率比第一种低；而第三种情况涉及到数据并行以外的并行方式（模型并行，张量并行，流水线并行等），相对更加复杂，不同模型之间的实现方法也各不相同，因此不作介绍。</p>
<blockquote>
<p>参考资料：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/178402798">DDP 系列第一篇：入门教程 - 知乎</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/187610959">DDP 系列第二篇：实现原理与源代码解析 - 知乎</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/250471767">DDP 系列第三篇：实战与技巧 - 知乎</a></li>
</ul>
</blockquote>
<h3 id="代码实现-2"><a class="anchor" href="#代码实现-2">#</a> 代码实现</h3>
<p>接着介绍 DDP 并行训练的实现过程。</p>
<p>首先定义数据加载器  <code>get_ddp_dataset_loader</code> ，与 DP 的数据加载器不同的是，这里使用了  <code>torch.utils.data.distributed.DistributedSampler</code>  来划分数据集，使得每个进程只处理自己负责的那一部分数据。此时  <code>DataLoader</code>  中的  <code>batch_size</code>  指的是每个进程处理的样本数量，并且不需要设置  <code>shuffle=True</code> ，因为  <code>DistributedSampler</code>  会自动处理数据的打乱。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># train.py</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">def</span> <span class="token function">get_ddp_dataset_loader</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    data <span class="token operator">=</span> np<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"data.npz"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    feature <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">"feature"</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    label <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    feature <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>feature<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    label <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>label<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    dataset <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>feature<span class="token punctuation">,</span> label<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    dataset_sampler <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>DistributedSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>  <span class="token comment"># 新增</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    data_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="11"></td><td><pre>                                              sampler<span class="token operator">=</span>dataset_sampler<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre>    <span class="token keyword">return</span> data_loader</pre></td></tr></tbody></table></figure><p>接着定义  <code>train_ddp</code> ，封装 DDP 的训练函数：</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel <span class="token keyword">import</span> DistributedDataParallel <span class="token keyword">as</span> DDP</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">def</span> <span class="token function">train_ddp</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    <span class="token comment"># 初始化分布式环境</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span><span class="token string">"nccl"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    local_rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">"LOCAL_RANK"</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>local_rank<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    <span class="token comment"># 构造模型</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    model <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span>local_rank<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    ckpt_path <span class="token operator">=</span> <span class="token boolean">None</span></pre></td></tr><tr><td data-num="11"></td><td><pre>    <span class="token keyword">if</span> dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> ckpt_path <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="12"></td><td><pre>        model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>ckpt_path<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="13"></td><td><pre>    model <span class="token operator">=</span> DDP<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 这里使用 DDP 包装模型</span></pre></td></tr><tr><td data-num="14"></td><td><pre>    data_loader <span class="token operator">=</span> get_ddp_dataset_loader<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="15"></td><td><pre>    <span class="token comment"># 用 DDP model 初始化 optimizer。</span></pre></td></tr><tr><td data-num="16"></td><td><pre>    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="17"></td><td><pre>    loss_fn <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="18"></td><td><pre></pre></td></tr><tr><td data-num="19"></td><td><pre>    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="20"></td><td><pre>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="21"></td><td><pre>        data_loader<span class="token punctuation">.</span>sampler<span class="token punctuation">.</span>set_epoch<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>  <span class="token comment"># 每个 epoch 调用，打乱数据</span></pre></td></tr><tr><td data-num="22"></td><td><pre>        epoch_loss <span class="token operator">=</span> <span class="token number">0.0</span></pre></td></tr><tr><td data-num="23"></td><td><pre>        num_batches <span class="token operator">=</span> <span class="token number">0</span></pre></td></tr><tr><td data-num="24"></td><td><pre>        <span class="token keyword">for</span> _<span class="token punctuation">,</span> <span class="token punctuation">(</span>feature<span class="token punctuation">,</span> label<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="25"></td><td><pre>            feature <span class="token operator">=</span> feature<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span>local_rank<span class="token punctuation">,</span> non_blocking<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># 数据放到对应 GPU</span></pre></td></tr><tr><td data-num="26"></td><td><pre>            label <span class="token operator">=</span> label<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span>local_rank<span class="token punctuation">,</span> non_blocking<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="27"></td><td><pre>            output <span class="token operator">=</span> model<span class="token punctuation">(</span>feature<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="28"></td><td><pre>            loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> label<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="29"></td><td><pre>            </pre></td></tr><tr><td data-num="30"></td><td><pre>            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="31"></td><td><pre>            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 内部自动完成梯度同步</span></pre></td></tr><tr><td data-num="32"></td><td><pre>            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="33"></td><td><pre>            epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="34"></td><td><pre>            num_batches <span class="token operator">+=</span> <span class="token number">1</span></pre></td></tr><tr><td data-num="35"></td><td><pre></pre></td></tr><tr><td data-num="36"></td><td><pre>        epoch_loss <span class="token operator">/=</span> num_batches</pre></td></tr><tr><td data-num="37"></td><td><pre>        loss_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>epoch_loss<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span>local_rank<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="38"></td><td><pre>        dist<span class="token punctuation">.</span>all_reduce<span class="token punctuation">(</span>loss_tensor<span class="token punctuation">,</span> op<span class="token operator">=</span>dist<span class="token punctuation">.</span>ReduceOp<span class="token punctuation">.</span>SUM<span class="token punctuation">)</span>  <span class="token comment"># 使用 all_reduce 计算全局 loss</span></pre></td></tr><tr><td data-num="39"></td><td><pre>        loss_avg <span class="token operator">=</span> loss_tensor<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="40"></td><td><pre></pre></td></tr><tr><td data-num="41"></td><td><pre>        <span class="token keyword">if</span> local_rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="42"></td><td><pre>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Epoch </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token punctuation">}</span></span><span class="token string"> - Global Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>loss_avg<span class="token punctuation">:</span><span class="token format-spec">.6f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="43"></td><td><pre></pre></td></tr><tr><td data-num="44"></td><td><pre>    dist<span class="token punctuation">.</span>destroy_process_group<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 清理分布式环境</span></pre></td></tr><tr><td data-num="45"></td><td><pre></pre></td></tr><tr><td data-num="46"></td><td><pre><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="47"></td><td><pre>    train_dp<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p>相比于 DP，DDP 的训练函数有以下几点不同：</p>
<ol>
<li>初始化分布式环境：代码最开始需要使用  <code>dist.init_process_group("nccl")</code>  来初始化分布式训练环境，指定通信后端为 NCCL（适用于 GPU 之间的高效通信）。各个进程会在这一步，与 master 节点进行握手，建立连接（单机环境默认 master 是第一个进程即 global rank 0）。</li>
<li>设置本地 GPU：通过读取环境变量  <code>LOCAL_RANK</code>  来确定当前进程使用的 GPU，并调用  <code>torch.cuda.set_device(local_rank)</code>  设置当前进程的默认 GPU。</li>
<li>使用 DDP 包装模型：将模型移动到对应的 GPU 上，并使用  <code>DistributedDataParallel</code>  包装模型，传入<strong>当前进程</strong>的 GPU ID。如果需要从某个 checkpoint 继续训练，那么需要在构造 DDP 模型之前使用  <code>load_state_dict</code> ，并且只需要在 master 上加载。</li>
<li>构造 DDP 模型后，再初始化对应的优化器。</li>
<li>数据加载器使用  <code>DistributedSampler</code> ：确保每个进程只处理自己负责的数据子集，并在每个 epoch 开始时调用  <code>data_loader.sampler.set_epoch(epoch)</code>  来打乱数据，确保进程间的数据不重复。</li>
<li>训练过程中，每个进程都在对应的 GPU 上进行训练，因此数据也必须放到对应的 GPU 中。</li>
<li>使用  <code>dist.all_reduce</code>  来在所有进程间同步数据，比如计算全局的平均损失。</li>
<li>仅在主进程（ <code>local_rank == 0</code> ）打印日志，避免重复输出。</li>
<li>如果需要保存模型（上述示例中未包含保存模型的代码），只需在主进程上使用  <code>model.module.state_dict()</code>  来获取模型参数。其中  <code>model.module</code>  是因为 DDP 包装后的模型，实际的模型在  <code>module</code>  属性中。</li>
<li>训练结束后调用  <code>dist.destroy_process_group()</code>  清理分布式环境。</li>
</ol>
<p>上述脚本有两种运行方法：</p>
<ol>
<li>
<p><code>torch.distributed.launch</code> ：早期启动方法，<strong>基本已弃用</strong>。</p>
<figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>python <span class="token parameter variable">-m</span> torch.distributed.launch <span class="token parameter variable">--nproc_per_node</span><span class="token operator">=</span><span class="token number">4</span> train.py</pre></td></tr></tbody></table></figure></li>
<li>
<p><code>torchrun</code> ：<strong>官方推荐</strong>的启动方式，它是对旧的  <code>torch.distributed.launch</code>  的替代。</p>
<figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0,1</span>,2,3 torchrun <span class="token parameter variable">--nproc_per_node</span><span class="token operator">=</span><span class="token number">4</span> train.py</pre></td></tr></tbody></table></figure><p><code>CUDA_VISIBLE_DEVICES</code>  用于指定可见的 GPU 设备， <code>--nproc_per_node</code>  指定每个节点使用的 GPU 数量，会自动设置每个进程的  <code>RANK</code> 、  <code>LOCAL_RANK</code>  等环境变量，一般与  <code>CUDA_VISIBLE_DEVICES</code>  中的 GPU 数量一致。</p>
<p>对于<strong>多机多卡</strong>， <code>torchrun</code>  命令需要指定更多参数：</p>
<figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># 节点 0 </span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0,1</span>,2,3 torchrun <span class="token parameter variable">--nnodes</span><span class="token operator">=</span><span class="token number">2</span> <span class="token parameter variable">--nproc_per_node</span><span class="token operator">=</span><span class="token number">4</span> <span class="token parameter variable">--node_rank</span><span class="token operator">=</span><span class="token number">0</span> <span class="token parameter variable">--master_addr</span><span class="token operator">=</span><span class="token string">"192.168.0.10"</span>  <span class="token parameter variable">--master_port</span><span class="token operator">=</span><span class="token number">23456</span> train.py</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token comment"># 节点 1</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0,1</span>,2,3 torchrun <span class="token parameter variable">--nnodes</span><span class="token operator">=</span><span class="token number">2</span> <span class="token parameter variable">--nproc_per_node</span><span class="token operator">=</span><span class="token number">4</span> <span class="token parameter variable">--node_rank</span><span class="token operator">=</span><span class="token number">1</span> <span class="token parameter variable">--master_addr</span><span class="token operator">=</span><span class="token string">"192.168.0.10"</span>  <span class="token parameter variable">--master_port</span><span class="token operator">=</span><span class="token number">23456</span> train.py</pre></td></tr></tbody></table></figure><p>其中  <code>--nnodes=2</code>  表示总共有 2 台机器， <code>--nproc_per_node=4</code> ：指定每台机器用 4 张 GPU； <code>--node_rank</code>  指定当前机器是第几台（从 0 开始）； <code>--master_addr</code>  为主节点 IP（一般写 node 0 的 IP）； <code>--master_port</code>  为主节点的通信端口。</p>
</li>
</ol>
<h3 id="补充说明"><a class="anchor" href="#补充说明">#</a> 补充说明</h3>
<p>至此，DDP 的并行训练的基本流程就介绍完了，下面再介绍一些 DDP 的其他相关知识。</p>
<p>首先来说明 PyTorch 模型中一类特殊的参数 Buffer，它们不属于 Parameter，反向传播计算过程中也<strong>不会计算这些参数的梯度</strong>，因此 Buffer 参数是不通过梯度进行更新的。常见的 Buffer 有 BatchNorm 中的  <code>running_mean</code> ， <code>running_var</code> ，它们在训练计算时直接由计算规则更新。</p>
<p>那么存在一个问题，对于 DDP 来说，每个模型事实上都维护了自己的 Buffer 参数，而 Buffer 参数的更新取决于当前的训练数据，它不像梯度一样会被广播，这将导致不同进程间的模型状态不一致。对于这个问题，DDP 内部的实现方法是：直接将 master 上的模型 Buffer 广播复制给其他进程。不难想到，这么做存在一个问题：<strong>如果 Buffer 依赖于数据分布，那么 master 上的 Buffer 很可能和其他进程上的 Buffer 差异较大，直接广播会导致其他进程的 Buffer 变得不准确</strong>。所以 PyTorch 提供  <code>SyncBatchNorm</code>  来解决这个问题。</p>
<p><code>SyncBatchNorm</code>  中各卡先本地算出自己的 mean /var，然后通过  <code>all_reduce</code>  算出全局的 mean /var，最后所有卡使用这个全局的 mean /var 来正则化激活，这样就保证了各卡的 BatchNorm 统计量更准确，训练更稳定。核心代码如下：在初始化 DDP 模型之前，使用  <code>nn.SyncBatchNorm.convert_sync_batchnorm</code>  函数搜索 model 里面的每一个模块，将  <code>BatchNorm</code>  替换成 <code>SyncBatchNorm</code> 。</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>model <span class="token operator">=</span> nn<span class="token punctuation">.</span>SyncBatchNorm<span class="token punctuation">.</span>convert_sync_batchnorm<span class="token punctuation">(</span>model<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>model <span class="token operator">=</span> DDP<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p>此外，对于分支模型（例如 MoE 混合专家模型）来说，不同进程上可能会激活不同的分支，这样就会导致不同进程反向传播时模型激活的参数不一样，从而导致梯度不一致。对于这种情况，默认配置的 DDP 模型会报错，提示  <code>Expected to have gradients for all parameters</code> 。解决方法是使用  <code>find_unused_parameters=True</code>  参数来初始化 DDP 模型：</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>model <span class="token operator">=</span> DDP<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">,</span> find_unused_parameters<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p>接下来我们探讨 Gradient Accumulation 的情况。在单卡情况下，将几个小批次的 loss 累计起来，等到累计到一定数量后再进行反向传播和参数更新，这样就相当于使用了更大的批次进行训练。对于单卡来说，代码大致是这样的：</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>feature<span class="token punctuation">,</span> label<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    output <span class="token operator">=</span> model<span class="token punctuation">(</span>feature<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> label<span class="token punctuation">)</span> <span class="token operator">/</span> accumulation_steps  <span class="token comment"># 归一化</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 累计梯度</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> accumulation_steps <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="7"></td><td><pre>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 更新参数</span></pre></td></tr><tr><td data-num="8"></td><td><pre>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 清空梯度</span></pre></td></tr></tbody></table></figure><p>但是对于 DDP 来说， <code>loss_fn</code>  内部会进行  <code>all_reduce</code>  操作来同步梯度，因此如果直接使用上述代码，会导致每个小批次的梯度会通讯一次，这导致计算效率非常低下。正确的做法是使用  <code>no_sync</code>  上下文管理器来跳过中间小批次的梯度同步，只有在最后一个小批次时才进行梯度同步：</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>feature<span class="token punctuation">,</span> label<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token keyword">with</span> model<span class="token punctuation">.</span>no_sync<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> accumulation_steps <span class="token operator">!=</span> <span class="token number">0</span> <span class="token keyword">else</span> contextlib<span class="token punctuation">.</span>nullcontext<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="4"></td><td><pre>        output <span class="token operator">=</span> model<span class="token punctuation">(</span>feature<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> label<span class="token punctuation">)</span> <span class="token operator">/</span> accumulation_steps  <span class="token comment"># 归一化</span></pre></td></tr><tr><td data-num="6"></td><td><pre>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 累计梯度</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> accumulation_steps <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="8"></td><td><pre>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 更新参数</span></pre></td></tr><tr><td data-num="9"></td><td><pre>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 清空梯度</span></pre></td></tr></tbody></table></figure><p><img loading="lazy" data-src="https://jongsh.oss-cn-beijing.aliyuncs.com/blog/content/202510/202510121643142.jpg?x-oss-process=image/format,webp" alt="Gradient Accumulation" title="Gradient Accumulation" width="650"></p>
<p>最后再来讨论 DDP 代码中的一个注意事项，也是易错点，即不同进程间随机种子的设定。一般来说，不同进程为了保证状态统一一般会考虑使用相同的随机种子，比如数据 Sampler 在每个 epoch 会使用相同的种子（epoch）进行划分。<strong>但是有个情况不应该使用相同的随机种子，那就是每个进程各自需要增强 / 采样数据的时候</strong>。比如在图像处理中，每个进程可能会从数据集中采样图像进行增强（旋转，擦除等），亦或者在强化学习中不同进程需要各自与环境交互获取经验，这些情况下，如果进程的随机种子设置一样，得到的数据也相同，那数据的利用会变得不均匀，导致训练效果变差。</p>
<p>下面写一个统一的管理随机种子的函数：</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">setup_seed</span><span class="token punctuation">(</span>base_seed<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> rank<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    seed <span class="token operator">=</span> base_seed <span class="token operator">+</span> rank</pre></td></tr><tr><td data-num="3"></td><td><pre>    torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>manual_seed_all<span class="token punctuation">(</span>seed<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p>其中  <code>base_seed</code>  是基本的种子，一般是脚本传入的参数， <code>rank</code>  对应不同进程编号。当我们需要使用不同的随机数时，传入进程的  <code>rank</code> ，当需要使用相同的随机数时，直接传入  <code>rank=0</code> 。不过一般来说，不同进程直接使用不同的随机种子不会对其他部分产生影响，这也是推荐做法。</p>
<br> 
<h2 id="accelerate"><a class="anchor" href="#accelerate">#</a> Accelerate</h2>
<p>Accelerate 是一个由 Hugging Face 提供的高效工具库，意在简化 PyTorch 模型在各种设备和分布式配置上的训练与推理。通过少量代码即可实现分布式训练、混合精度训练，以及与 DeepSpeed 和 FSDP 的集成。</p>
<blockquote>
<p>本文仅介绍核心的基本的使用方法，一些高级功能如 DeepSpeed、Megatron-LM、FSDP 的集成和细节请参考<a target="_blank" rel="noopener" href="https://hugging-face.cn/docs/accelerate/index">官方文档</a></p>
</blockquote>
<p>相比于 DDP 需要管理分布式环境、进程间通信等细节，Accelerate 提供了更高层次的抽象，用户只需关注模型和训练逻辑，而不必处理底层的分布式实现细节。相比于 DDP，Accelerate 有如下特点：</p>
<ul>
<li>不必手动设置  <code>torch.distributed</code> 。</li>
<li>自动包装 DDP  <code>DistributedDataParallel</code> 。</li>
<li>自动处理  <code>DataLoader</code>  的  <code>DistributedSampler</code> 。</li>
<li>支持梯度累积、混合精度 (FP16/ BF16)。</li>
</ul>
<p>下面来看 Accelerate 相关代码。</p>
<p>首先定义数据加载器  <code>get_acc_dataset_loader</code> ，其实现方式与 DP 几乎一致，<strong>不同的是这里的  <code>batch_size</code>  指的是每个进程处理的样本数量</strong>。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DistributedSampler</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">def</span> <span class="token function">get_acc_dataset_loader</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    data <span class="token operator">=</span> np<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"data.npz"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    feature <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">"feature"</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    label <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    feature <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>feature<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    label <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>label<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    dataset <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>feature<span class="token punctuation">,</span> label<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    data_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    <span class="token keyword">return</span> data_loader</pre></td></tr></tbody></table></figure><p>接下来写训练函数  <code>train_acc</code> :</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">train_acc</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    accelerator <span class="token operator">=</span> accelerate<span class="token punctuation">.</span>Accelerator<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 初始化 Accelerator</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    model <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    data_loader <span class="token operator">=</span> get_acc_dataset_loader<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre>    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    loss_fn <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre>    <span class="token comment"># 使用 accelerator 准备模型、优化器和数据加载器</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    model<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> data_loader <span class="token operator">=</span> accelerator<span class="token punctuation">.</span>prepare<span class="token punctuation">(</span>model<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> data_loader<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre></pre></td></tr><tr><td data-num="12"></td><td><pre>    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="13"></td><td><pre>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="14"></td><td><pre>        epoch_loss <span class="token operator">=</span> <span class="token number">0.0</span></pre></td></tr><tr><td data-num="15"></td><td><pre>        num_batches <span class="token operator">=</span> <span class="token number">0</span></pre></td></tr><tr><td data-num="16"></td><td><pre>        <span class="token keyword">for</span> feature<span class="token punctuation">,</span> label <span class="token keyword">in</span> data_loader<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="17"></td><td><pre>            <span class="token comment"># 将数据转到正确设备</span></pre></td></tr><tr><td data-num="18"></td><td><pre>            feature <span class="token operator">=</span> feature<span class="token punctuation">.</span>to<span class="token punctuation">(</span>accelerator<span class="token punctuation">.</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="19"></td><td><pre>            label <span class="token operator">=</span> label<span class="token punctuation">.</span>to<span class="token punctuation">(</span>accelerator<span class="token punctuation">.</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="20"></td><td><pre></pre></td></tr><tr><td data-num="21"></td><td><pre>            output <span class="token operator">=</span> model<span class="token punctuation">(</span>feature<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="22"></td><td><pre>            loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> label<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="23"></td><td><pre></pre></td></tr><tr><td data-num="24"></td><td><pre>            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="25"></td><td><pre>            accelerator<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>  <span class="token comment"># 使用 accelerator 的 backward 方法</span></pre></td></tr><tr><td data-num="26"></td><td><pre>            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="27"></td><td><pre></pre></td></tr><tr><td data-num="28"></td><td><pre>            epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="29"></td><td><pre>            num_batches <span class="token operator">+=</span> <span class="token number">1</span></pre></td></tr><tr><td data-num="30"></td><td><pre></pre></td></tr><tr><td data-num="31"></td><td><pre>        epoch_loss <span class="token operator">/=</span> num_batches</pre></td></tr><tr><td data-num="32"></td><td><pre></pre></td></tr><tr><td data-num="33"></td><td><pre>        <span class="token comment"># 多卡聚合 loss</span></pre></td></tr><tr><td data-num="34"></td><td><pre>        epoch_loss_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>epoch_loss<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>accelerator<span class="token punctuation">.</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="35"></td><td><pre>        accelerator<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>epoch_loss_tensor<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="36"></td><td><pre></pre></td></tr><tr><td data-num="37"></td><td><pre>        <span class="token keyword">if</span> accelerator<span class="token punctuation">.</span>is_main_process<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="38"></td><td><pre>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Epoch </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token punctuation">}</span></span><span class="token string"> - Global Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_loss_tensor<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.6f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="39"></td><td><pre></pre></td></tr><tr><td data-num="40"></td><td><pre>    dist<span class="token punctuation">.</span>destroy_process_group<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 清理分布式环境</span></pre></td></tr></tbody></table></figure><p>由于 Accelerate 本质是对 DDP 的更高一级的封装，所以一些使用细节和 DDP 代码是类似的，总结如下：</p>
<ol>
<li>使用  <code>accelerate.Accelerator()</code>  初始化对象。</li>
<li>使用  <code>accelerator.prepare()</code>  准备模型、优化器和数据加载器</li>
<li>训练过程中，每个进程都在对应的 GPU 上进行训练，因此数据也必须放到对应的 GPU 中（使用  <code>accelerator.device</code>  获取当前进程分配的 GPU）。</li>
<li>使用  <code>accelerator.backward(loss)</code>  计算梯度，内部自动完成梯度同步。</li>
<li>仅在主进程打印日志，避免重复输出。Accelerate 提供了  <code>accelerator.is_main_process</code>  来判断当前进程是否为主进程。</li>
<li>如果需要保存模型（上述示例中未包含保存模型的代码），只需在主进程上使用  <code>accelerator.save(model.state_dict(), path)</code>  来保存模型参数。</li>
<li>训练结束后调用  <code>dist.destroy_process_group()</code>  清理分布式环境。</li>
</ol>
<p>上述代码通过下面脚本运行：</p>
<figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>accelerate config  <span class="token comment"># 第一次使用需要配置</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0,1</span> accelerate launch <span class="token parameter variable">--num_processes</span><span class="token operator">=</span><span class="token number">2</span> train.py</pre></td></tr></tbody></table></figure><p>如果第一次使用 Accelerate 库，需要先进行配置，生成对应的配置文件，之后就不再需要执行配置命令。对于多卡训练，使用  <code>–num_processes</code>  指定使用的 GPU 数量，一个进程对应一个 GPU。如果需要指定使用哪些 GPU，和 DDP 类似，就使用  <code>CUDA_VISIBLE_DEVICES</code>  环境变量来声明。要注意与  <code>--num_processes</code>  保持一致。</p>
<p>Accelerate 实现了更高层次的封装，优点在于高效和简洁，但缺点也很明显，就是不方便自定义设置，特别是某些特定场景下无法使用封装接口的情况（比如强化学习）。</p>
<br> 
<h2 id="pytorch-分布式-api"><a class="anchor" href="#pytorch-分布式-api">#</a> PyTorch 分布式 API</h2>
<p>前面的各种训练方案，许多实现细节都被封装起来了，但它们内部都使用了  <code>torch.distributed</code>  的 API 来实现分布式训练。为方便查缺补漏，接下来介绍一些常用的 PyTorch 分布式 API。</p>
<h3 id="初始化分布式环境与清理"><a class="anchor" href="#初始化分布式环境与清理">#</a> 初始化分布式环境与清理</h3>
<p>标准的初始化分布式环境和清除函数：</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> torch</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">as</span> dist</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">import</span> os</pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token keyword">def</span> <span class="token function">setup</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    local_rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'LOCAL_RANK'</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>local_rank<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="9"></td><td><pre>        backend<span class="token operator">=</span><span class="token string">'nccl'</span><span class="token punctuation">,</span>          <span class="token comment"># GPU 上常用 'nccl'，CPU 可用 'gloo'</span></pre></td></tr><tr><td data-num="10"></td><td><pre>        rank<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'RANK'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="11"></td><td><pre>        world_size<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'WORLD_SIZE'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="12"></td><td><pre>        device_id<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"cuda:</span><span class="token interpolation"><span class="token punctuation">{</span>local_rank<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="13"></td><td><pre>    <span class="token punctuation">)</span></pre></td></tr><tr><td data-num="14"></td><td><pre></pre></td></tr><tr><td data-num="15"></td><td><pre><span class="token keyword">def</span> <span class="token function">cleanup</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="16"></td><td><pre>    dist<span class="token punctuation">.</span>destroy_process_group<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p><code>setup</code>  函数在分布式训练开始时调用， <code>cleanup</code>  函数在训练结束时调用。</p>
<h3 id="分布式变量"><a class="anchor" href="#分布式变量">#</a> 分布式变量</h3>
<p>与分布式进程有关的变量核心有四个，对应的获取方法见下表。</p>
<table>
<thead>
<tr>
<th style="text-align:center">变量名</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">获取 API</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>rank</code></td>
<td style="text-align:center">全局进程 ID，唯一标识每个进程</td>
<td style="text-align:center"><code>os.environ["RANK"]</code>  或  <code>dist.get_rank()</code></td>
</tr>
<tr>
<td style="text-align:center"><code>local_rank</code></td>
<td style="text-align:center">当前节点内的进程编号</td>
<td style="text-align:center"><code>os.environ["LOCAL_RANK"]</code></td>
</tr>
<tr>
<td style="text-align:center"><code>world_size</code></td>
<td style="text-align:center">全局参与进程数</td>
<td style="text-align:center"><code>os.environ["WORLD_SIZE"]</code>  或  <code>dist.get_world_size()</code></td>
</tr>
<tr>
<td style="text-align:center"><code>node_rank</code></td>
<td style="text-align:center">当前机器节点编号</td>
<td style="text-align:center"><code>os.environ["NODE_RANK"]</code></td>
</tr>
</tbody>
</table>
<p><strong>上述变量获取后记得用  <code>int</code>  强转类型。</strong></p>
<h3 id="常用通信-api"><a class="anchor" href="#常用通信-api">#</a> 常用通信 API</h3>
<p>PyTorch 的  <code>torch.distributed</code>  提供了丰富的通信原语，常用于参数同步、统计指标计算、分布式采样等。</p>
<p><strong>send/recv</strong>：最基础的点对点通信方式，两个进程之间传输数据，一般很少直接使用。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>dist<span class="token punctuation">.</span>send<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> dst<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 将 tensor 发送到 rank=1</span></pre></td></tr><tr><td data-num="2"></td><td><pre>dist<span class="token punctuation">.</span>recv<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> src<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 从 rank=0 接收 tensor</span></pre></td></tr></tbody></table></figure><p>阻塞式通信，必须配对使用，否则程序会卡住。</p>
<p><strong>all_reduce</strong>：所有进程的向量进行运算，都拿到同样的结果。 <code>op</code>  有其他操作算子，不过最常见的是累加和平均。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>dist<span class="token punctuation">.</span>all_reduce<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> op<span class="token operator">=</span>dist<span class="token punctuation">.</span>ReduceOp<span class="token punctuation">.</span>SUM<span class="token punctuation">)</span>  <span class="token comment"># 或 .AVG</span></pre></td></tr></tbody></table></figure><p><strong>reduce</strong>：和  <code>all_reduce</code>  类似，但结果<strong>只给一个进程</strong>。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>dist<span class="token punctuation">.</span><span class="token builtin">reduce</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> dst<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> op<span class="token operator">=</span>dist<span class="token punctuation">.</span>ReduceOp<span class="token punctuation">.</span>SUM<span class="token punctuation">)</span> <span class="token comment"># 在 rank0 汇总</span></pre></td></tr></tbody></table></figure><p><strong>broadcast</strong>：从一个进程复制向量到所有进程。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>dist<span class="token punctuation">.</span>broadcast<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> src<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># rank0 想其他进程广播</span></pre></td></tr></tbody></table></figure><p><strong>all_gather</strong>：所有进程聚集所有数据，然后每个卡都拿到完整副本。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># all_gather 每个 GPU 收到完整数据列表</span></pre></td></tr><tr><td data-num="2"></td><td><pre>tensor_list <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>world_size<span class="token punctuation">)</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="3"></td><td><pre>dist<span class="token punctuation">.</span>all_gather<span class="token punctuation">(</span>tensor_list<span class="token punctuation">,</span> tensor<span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p><strong>gather</strong>：所有进程发送数据到目标进程，只有它接收。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre><span class="token comment"># gather 只有 rank0 收到</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">if</span> rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    gather_list <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>world_size<span class="token punctuation">)</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">else</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    gather_list <span class="token operator">=</span> <span class="token boolean">None</span></pre></td></tr><tr><td data-num="6"></td><td><pre>dist<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> gather_list<span class="token punctuation">,</span> dst<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr></tbody></table></figure><p><strong>barrier</strong>：等待所有进程都执行到此位置。</p>
<figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tbody><tr><td data-num="1"></td><td><pre>dist<span class="token punctuation">.</span>barrier<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></tbody></table></figure></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-eye"></i></span><span class="text">总访问量：</span><span class="waline-pageview-count" id="twikoo_visitors" data-path="/coding-diary/pytorch-note/pytorch-model-training-parallel/">加载中...</span></span><span class="item"><span class="icon"><i class="ic i-calendar-check"></i></span><span class="text">更新于 </span><time title="修改时间：2025-10-12 17:03:11" itemprop="dateModified" datetime="2025-10-12T17:03:11+08:00">2025-10-12</time></span></div><div id="copyright"><ul><li class="author"><strong>本文作者：</strong>Jongsh<i class="ic i-at"><em>@</em></i>穷拾の小屋</li><li class="link"><strong>本文链接：</strong><a href="https://blog.jongsh.top/coding-diary/pytorch-note/pytorch-model-training-parallel/" title="PyTorch 模型训练之并行篇">https://blog.jongsh.top/coding-diary/pytorch-note/pytorch-model-training-parallel/</a></li><li class="license"><strong>版权声明：</strong>本站所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/learning-note/reinforcement-learning/reinforcement-learning-math/" rel="prev" itemprop="url" data-background-image="https://jongsh.oss-cn-beijing.aliyuncs.com/blog/cover/202504181600850.jpg?x-oss-process=image/format,webp" title="强化学习的数学原理"><span class="type">上一篇</span><span class="category"><i class="ic i-flag"></i>强化学习</span><h3>强化学习的数学原理</h3></a></div><div class="item right"><a href="/stanford-cs336/bpe-tokenizer-algorithm/" rel="next" itemprop="url" data-background-image="https://jongsh.oss-cn-beijing.aliyuncs.com/blog/cover/202511271917561.png?x-oss-process=image/format,webp" title="BPE 分词器算法原理与实现"><span class="type">下一篇</span><span class="category"><i class="ic i-flag"></i>斯坦福 CS336 课程</span><h3>BPE 分词器算法原理与实现</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E6%9C%9F%E5%87%86%E5%A4%87"><span class="toc-number">1.</span> <span class="toc-text"> 前期准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#data-parallel"><span class="toc-number">2.</span> <span class="toc-text"> Data Parallel</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.1.</span> <span class="toc-text"> 原理介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.2.</span> <span class="toc-text"> 代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#distributed-data-parallel"><span class="toc-number">3.</span> <span class="toc-text"> Distributed Data Parallel</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D-2"><span class="toc-number">3.1.</span> <span class="toc-text"> 原理介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">3.2.</span> <span class="toc-text"> 代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E"><span class="toc-number">3.3.</span> <span class="toc-text"> 补充说明</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#accelerate"><span class="toc-number">4.</span> <span class="toc-text"> Accelerate</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch-%E5%88%86%E5%B8%83%E5%BC%8F-api"><span class="toc-number">5.</span> <span class="toc-text"> PyTorch 分布式 API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E4%B8%8E%E6%B8%85%E7%90%86"><span class="toc-number">5.1.</span> <span class="toc-text"> 初始化分布式环境与清理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%98%E9%87%8F"><span class="toc-number">5.2.</span> <span class="toc-text"> 分布式变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E9%80%9A%E4%BF%A1-api"><span class="toc-number">5.3.</span> <span class="toc-text"> 常用通信 API</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/coding-diary/pytorch-note/pytorch-model-training-basics/" rel="bookmark" title="PyTorch 模型训练之基础篇">PyTorch 模型训练之基础篇</a></li><li class="active"><a href="/coding-diary/pytorch-note/pytorch-model-training-parallel/" rel="bookmark" title="PyTorch 模型训练之并行篇">PyTorch 模型训练之并行篇</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><img class="image lozaded" loading="lazy" decoding="async" itemprop="image" alt="Jongsh" src="/assets/avatar.avif"><p class="name" itemprop="name">Jongsh</p><div class="description" itemprop="description">Talk is cheap, show me the code!</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">12</span><span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">8</span><span class="name">分类</span></a></div></nav><div class="social"><a target="_blank" rel="noopener" href="https://github.com/jongsh" class="item github" title="https://github.com/jongsh"><i class="ic i-github"></i></a><a target="_blank" rel="noopener" href="https://gitee.com/jongsh" class="item gitee" title="https://gitee.com/jongsh"><i class="ic i-gitee"></i></a><a href="/cjh1967662798@outlook.com" class="item envelope" title="cjh1967662798@outlook.com"><i class="ic i-envelope"></i></a></div><div class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="#" onclick="return false;"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友链</a></li></div></div></div></div><ul id="quick"><li class="prev pjax"><a href="/stanford-cs336/bpe-tokenizer-algorithm/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/learning-note/reinforcement-learning/reinforcement-learning-math/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div><div id="player"></div></main></div><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/coding-diary/" title="分类于编程日常">编程日常</a><i class="ic i-angle-right"></i><a href="/categories/coding-diary/pytorch-note/" title="分类于PyTorch 学习笔记">PyTorch 学习笔记</a></div><span><a href="/coding-diary/pytorch-note/pytorch-model-training-basics/">PyTorch 模型训练之基础篇</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/learning-note/" title="分类于学习笔记">学习笔记</a><i class="ic i-angle-right"></i><a href="/categories/learning-note/frontend-development/" title="分类于前端开发">前端开发</a></div><span><a href="/learning-note/frontend-development/vue3-project-build-guide/">Vue3 项目构建指南</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/announcement/">网站迁移完成</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/learning-note/" title="分类于学习笔记">学习笔记</a><i class="ic i-angle-right"></i><a href="/categories/learning-note/reinforcement-learning/" title="分类于强化学习">强化学习</a></div><span><a href="/learning-note/reinforcement-learning/reinforcement-learning-math/">强化学习的数学原理</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/blog-site/" title="分类于博客网站">博客网站</a></div><span><a href="/blog-site/image-hosting-tips/">图床那点事</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/blog-site/" title="分类于博客网站">博客网站</a></div><span><a href="/blog-site/cabin-building-record/">小屋搭建记录</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/learning-note/" title="分类于学习笔记">学习笔记</a><i class="ic i-angle-right"></i><a href="/categories/learning-note/server-configuration/" title="分类于服务器配置">服务器配置</a></div><span><a href="/learning-note/server-configuration/linux-nfs-cifs/">Linux 环境配置 NFS 与 CIFS</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/coding-diary/" title="分类于编程日常">编程日常</a></div><span><a href="/coding-diary/weibo-poi-crawler/">微博 POI 数据爬取</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/coding-diary/" title="分类于编程日常">编程日常</a><i class="ic i-angle-right"></i><a href="/categories/coding-diary/pytorch-note/" title="分类于PyTorch 学习笔记">PyTorch 学习笔记</a></div><span><a href="/coding-diary/pytorch-note/pytorch-model-training-parallel/">PyTorch 模型训练之并行篇</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/stanford-cs336/" title="分类于斯坦福 CS336 课程">斯坦福 CS336 课程</a></div><span><a href="/stanford-cs336/bpe-tokenizer-algorithm/">BPE 分词器算法原理与实现</a></span></li></ul></div><div class="rpost pjax"><h2>最新评论</h2><ul class="leancloud-recent-comment" id="new-comment"></ul></div></div><div class="status"><div class="copyright">© 2024 -<span itemprop="copyrightYear">2026</span><span class="with-love"><i class="ic i-sakura rotate"></i></span><span class="author" itemprop="copyrightHolder">Jongsh @ JONGSH'S BLOG</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i></span><span title="站点总字数">154k 字</span><span class="post-meta-divider"> | </span><span class="post-meta-item-icon"><i class="ic i-coffee"></i></span><span title="站点阅读时长">2:20</span></div><div class="powered-by">基于 <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> &amp; Theme.<a target="_blank" rel="noopener" href="https://github.com/theme-shoka-x/hexo-theme-shokaX/">ShokaX</a></div></div></div></footer></div><script data-config="" type="text/javascript">var LOCAL = {
    ispost: true,
    path: `coding-diary/pytorch-note/pytorch-model-training-parallel/`,
    favicon: {
        show: `(๑◔‿◔๑)被我骗了吧~`,
        hide: `(×_×) 404网站未响应`
    },
    search: {
        placeholder: "文章搜索",
        empty: "关于 「 ${query} 」，什么也没搜到",
        stats: "${time} ms 内找到 ${hits} 条结果"
    },
    nocopy: "false",
    copyright: `复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。`,
    copy_tex: false,
    katex: false,
    mermaid: false,
    audio: undefined,
    fancybox: true,
    nocopy: false,
    outime: true,
    template: `<div class="note warning"><p><span class="label warning">文章时效性提示</span><br>这是一篇发布于 {{publish}} 天前，最后一次更新在 {{updated}} 天前的文章，部分信息可能已经发生改变，请注意甄别。</p></div>`,
    quiz: {
        choice: `单选题`,
        multiple: `多选题`,
        true_false: `判断题`,
        essay: `问答题`,
        gap_fill: `填空题`,
        mistake: `错题备注`
    },
    ignores: [
        (uri) => uri.includes('#'),
        (uri) => new RegExp(LOCAL.path + '$').test(uri),
            []
    ]
};
</script><script src="https://s4.zstatic.net/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha384-k6YtvFUEIuEFBdrLKJ3YAUbBki333tj1CSUisai5Cswsg9wcLNaPzsTHDswp4Az8" crossorigin="anonymous" fetchpriority="high"></script><script src="https://s4.zstatic.net/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous" fetchpriority="high"></script><script src="https://s4.zstatic.net/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha384-Zm+UU4tdcfAm29vg+MTbfu//q5B/lInMbMCr4T8c9rQFyOv6PlfQYpB5wItcXWe7" crossorigin="anonymous" fetchpriority="high"></script><script src="https://s4.zstatic.net/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" integrity="sha384-TOxsBplaL96/QDWPIUg+ye3v89qSE3s22XNtJMmCeZEep3cVDmXy1zEfZvVv+y2m" crossorigin="anonymous" fetchpriority="high"></script><script src="/js/siteInit.js?v=0.4.25" type="module" fetchpriority="high" defer=""></script></body></html>